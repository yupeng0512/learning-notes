# VideoCut-Skills 精读笔记：AI 口播视频剪辑 Skill

> 学习时间：2026-01-25
> 来源：https://github.com/Ceeon/videocut-skills
> 核心价值：AI 自动识别口误/静音/语气词，生成审查稿，一键剪辑口播视频

---

## 根节点命题

**口播视频剪辑的瓶颈在「识别多余内容」——AI 用时间戳驱动的方法论，实现转录→识别→审核→剪辑的完整自动化流程。**

---

## 问题本质

```
口播视频剪辑痛点
    │
    ├─→ 口误识别：说错重说、卡顿、语气词难以手动定位
    │
    ├─→ 时间精度：删除必须精确到毫秒级，否则剪断说话
    │
    └─→ 流程繁琐：转录→标注→剪辑，每步都要人工

解法：时间戳驱动 + AI 识别 + 人工审核 + 自动执行
```

---

## 架构全景图

```
VideoCut-Skills 系统
│
├── 📦 安装（videocut:安装）
│   ├── Python 依赖：funasr, modelscope, openai-whisper
│   ├── FFmpeg：视频剪辑底层
│   └── 模型下载：约 5GB（FunASR 2GB + Whisper 3GB）
│
├── 🎙️ 剪口播（videocut:剪口播）
│   │
│   ├── 流程
│   │   ├── Step 1: FunASR 30s 分段转录（字符级时间戳）
│   │   ├── Step 2: 识别口误（逐句检查）
│   │   ├── Step 3: VAD 检测微口误（短片段 < 0.5s）
│   │   ├── Step 4: 扫描语气词（嗯/哎/诶）
│   │   ├── Step 5: 识别静音（≥1s）
│   │   ├── Step 6: 生成审查稿（时间戳驱动）
│   │   └── Step 7: 输出删除任务 TodoList → 等用户确认
│   │
│   └── 核心文件
│       ├── tips/口误识别方法论.md
│       └── tips/转录最佳实践.md
│
├── ✂️ 剪辑（videocut:剪辑）
│   └── 用户确认后，根据时间戳执行 FFmpeg 剪辑
│
├── 📝 字幕（videocut:字幕）
│   ├── Whisper 转录 → 词典纠错 → 用户审核 → 烧录
│   └── 规范：一屏一行、≤15字、句尾无标点
│
└── 🔄 自更新（videocut:自更新）
    └── 从错误中学习，更新方法论（规则整合到正文，不只加末尾）
```

---

## 核心机制精解

### 机制 1：时间戳驱动的剪辑哲学

**为什么是「时间戳驱动」？**

```
❌ 文本搜索剪辑：搜"AI就是" → 可能匹配错误位置
✅ 时间戳剪辑：直接用 (85.78-86.50) → 精确无误
```

**删除任务格式**：

```markdown
口误（3处）：
- [ ] 1. `(1.36-2.54)` 删"你以为今天a" → 保留"你以为钉钉"
- [ ] 2. `(47.55-47.69)` 删"新" → 保留"拉满"

静音（2处）：
- [ ] 1. `(41.35-47.55)` 静音6.2s
```

### 机制 2：30s 分段转录解决漂移问题

**关键发现**：

| 方案 | 问题 |
|------|------|
| FunASR 全视频 | 长视频时间戳漂移（~10s/3分钟） |
| **FunASR 30s分段** | ✅ 无漂移 + 精确时间戳 |

**实现要点**：
- 30s 分段避免时间戳漂移
- `timestamp_granularity="character"` 获取字符级时间戳
- 每段结果加上段起始偏移

### 机制 3：口误类型与删除策略

| 类型 | 示例 | 删除策略 |
|------|------|----------|
| 重复型 | `拉满新拉满` | 只删差异部分（"新"） |
| 替换型 | `AI就是AI就会` | 删第一个完整版本（"AI就是"） |
| 卡顿型 | `听会会` | 删第一个重复字 |

**核心原则**：删前面，保后面

**识别方法**：逐句检查，不用正则

```
每句话问自己：
1. 句子完整吗？ → 残句 = 口误
2. 有重复吗？ → 词语/短语重复
3. 通顺吗？ → 不通顺 = 说错了
```

### 机制 4：微口误检测（VAD）

**什么是微口误**：不成词的卡顿音，转录检测不到

| 类型 | 示例 | 特点 |
|------|------|------|
| 起音卡顿 | "呃...你以为" | 开头有杂音 |
| 吞字 | "钉钉A...钉钉AI" | 说到一半重来 |
| 气口声 | 吸气/呼气声 | 换气太重 |

**检测方法**：用 FunASR VAD 找出所有发声片段

```
[0.000s - 0.430s] 语音活动  ← 微口误！
[0.710s - 4.980s] 语音活动  ← 正式语音
```

**判断规则**：短语音 + 长静音 + 长语音 → 第一个是微口误

### 机制 5：语气词删除的精确边界

**常见语气词**：嗯、啊、哎、诶、呃、额、唉、哦、呀

**删除边界要点**：

```
❌ 错误：删语气词的时间戳 (语气词.start - 语气词.end)
   → 可能删掉前面字的尾音

✅ 正确：从前一个字的 end 到后一个字的 start
   → (前字.end - 后字.start) 包含静音+语气词
```

### 机制 6：自更新的规则整合原则

**核心原则**：规则整合到正文，反馈记录只是事件日志

```
❌ 错误做法：
在反馈记录末尾新增3条教训 → 规则散落，下次还会犯

✅ 正确做法：
1. 读全文，理解章节结构
2. 找到相应位置，把规则整合进去
3. 反馈记录只记事件："审查稿标记了静音，但剪辑时漏删"
```

---

## 技术选型

| 组件 | 技术 | 用途 |
|------|------|------|
| 语音识别 | FunASR paraformer-zh | 中文转录 + 字符级时间戳 |
| VAD | FunASR fsmn-vad | 微口误检测 |
| 标点 | FunASR ct-punc | 自动加标点 |
| 字幕 | OpenAI Whisper large-v3 | 高质量字幕 |
| 剪辑 | FFmpeg | 视频处理 |

**模型总计**：约 5GB

| 模型 | 大小 |
|------|------|
| paraformer-zh | 953MB |
| punc_ct | 1.1GB |
| fsmn-vad | 4MB |
| whisper large-v3 | 2.9GB |

---

## 字幕规范

| 规则 | 说明 |
|------|------|
| 一屏一行 | 不换行，不堆叠 |
| ≤15字/行 | 超过15字必须拆分（4:3竖屏） |
| 句尾无标点 | `你好` 不是 `你好。` |
| 句中保留标点 | `先点这里，再点那里` |

**词典纠错**：用户只写正确写法，AI 自动识别变体

```
skills → 纠正 skill、SKILLS
Claude → 纠正 claude、CLAUDE
```

---

## 验证清单

口误处理检查：
- [ ] 删的是前面版本？
- [ ] 保留的文本完整通顺？
- [ ] 删除后不会产生新的重复？
- [ ] 时间戳精确到小数点后两位？

---

## 可迁移的设计模式

| 模式 | 说明 | 适用场景 |
|------|------|----------|
| **时间戳驱动** | 删除操作基于精确时间，不搜文本 | 音视频剪辑 |
| **分段处理** | 长内容分段避免漂移/溢出 | 长文本/长视频 |
| **先方法论后执行** | 必须读 tips 再行动 | 复杂任务 |
| **审查稿模式** | AI 生成 → 人工审核 → 确认执行 | 高风险操作 |
| **自更新机制** | 规则整合到正文，不只加末尾 | Skill 持续优化 |

---

## 延伸思考

1. **多语言支持**：FunASR 主攻中文，英文/方言如何处理？
2. **噪音场景**：背景音乐/环境噪音下识别效果如何？
3. **与其他 Skill 联动**：能否与 subtitle-generator 等字幕 Skill 打通？
4. **批量处理**：多视频批量剪辑的工作流设计？
