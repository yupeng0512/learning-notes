---
title: Manus 上下文工程：AI Agent 的六大实战原则
source: https://manus.im/zh-cn/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus
author: Yichao 'Peak' Ji
date: 2026-01-28
category: ai-tools/agent-architecture
tags:
  - AI Agent
  - 上下文工程
  - Context Engineering
  - KV-cache
  - Manus
  - 20亿美金
  - 实战方法论
  - Agent 优化
valuation: 20亿美金
learning_mode: 深度模式
---

# Manus 上下文工程：AI Agent 的六大实战原则

> Manus（20 亿美金估值）团队通过四次架构重建总结出的 AI Agent 上下文工程实战方法论。

## 📋 文章概览

| 维度 | 信息 |
|------|------|
| **作者** | Yichao 'Peak' Ji (Manus 创始人) |
| **公司估值** | 20 亿美金（现已被 Meta 收购）|
| **发布时间** | 2024-07-18 |
| **核心主题** | AI Agent 的上下文工程最佳实践 |
| **架构重建次数** | 4 次（"随机研究生下降"）|
| **核心指标** | KV-cache 命中率 |

### 核心价值

**一句话概括**：通过 6 条实战原则，教你如何设计高效的 AI Agent 上下文，实现 10 倍成本节省和更强的任务能力。

---

## 🚀 Phase 2P: 快速上手实践

### 核心决策：为什么选择上下文工程？

**历史教训**：

```
BERT 时代（2017）:
  - 每次模型微调需要数周
  - 快速迭代困难
  - 一旦 GPT-3 出现，内部模型一夜过时
  ↓
GPT-3 / Flan-T5 时代:
  - 上下文学习（ICL）兴起
  - 几小时内交付改进
  - 模型进步 = 上涨的潮水，产品 = 船（而非固定的柱子）
  ↓
Manus 的选择:
  - 押注上下文工程
  - 保持与底层模型正交
  - "随机研究生下降"（SGD）：手动架构搜索 + 提示调整
```

### Agent vs 聊天机器人：本质差异

| 维度 | Agent（Manus）| 聊天机器人 |
|------|--------------|-----------|
| **工作模式** | 工具调用链（多轮迭代）| 单轮对话 |
| **上下文增长** | 每步累积（动作 + 观察）| 相对稳定 |
| **输入/输出比** | 100:1（预填充 >> 解码）| 约 10:1 |
| **KV-cache 重要性** | ⭐⭐⭐⭐⭐（核心指标）| ⭐⭐⭐ |
| **核心挑战** | 上下文膨胀、工具爆炸 | 对话一致性 |

---

## 🌳 根节点命题

> **上下文工程 = KV-cache 优化（性能）+ 注意力操控（能力）+ 外部化记忆（扩展性）**

### 为什么这是根节点？

Manus 的上下文工程方法论本质是在**资源约束**（延迟、成本、上下文长度）下，通过**精心设计的上下文结构**最大化 Agent 的能力。

#### 三个核心维度

1. **KV-cache 优化（经济性）**
   ```
   问题：Agent 预填充/解码比 = 100:1，每次迭代都要重新处理上下文
     ↓
   解决：保持上下文前缀稳定 → 缓存命中 → 10 倍成本节省
     ↓
   衍生原则：只追加、稳定提示、显式缓存断点、遮蔽而非移除
   ```

2. **注意力操控（任务执行力）**
   ```
   问题：长上下文 + 多步任务 → 目标漂移、"丢失在中间"
     ↓
   解决：通过复述（todo.md）将目标推入近期注意力
     ↓
   衍生原则：自然语言偏置注意力、状态重述
   ```

3. **外部化记忆（扩展性）**
   ```
   问题：128K 上下文窗口不够 + 压缩会丢失信息
     ↓
   解决：文件系统 = 终极上下文（无限大小、天然持久）
     ↓
   衍生原则：可恢复压缩、文件作为外部记忆
   ```

#### 从根节点推导所有原则

```
根节点：资源约束下的上下文优化
    │
    ├─→ 经济性（KV-cache）
    │   ├─→ 原则 1: 围绕 KV-cache 设计
    │   └─→ 原则 2: 遮蔽而非移除工具
    │
    ├─→ 扩展性（突破上下文限制）
    │   └─→ 原则 3: 文件系统作为上下文
    │
    ├─→ 任务执行力（注意力管理）
    │   └─→ 原则 4: 复述操控注意力
    │
    └─→ 鲁棒性（错误恢复）
        ├─→ 原则 5: 保留错误内容
        └─→ 原则 6: 增加上下文多样性
```

所有 6 条原则都是从这个根节点自然展开的，没有一条是孤立的。

---

## 📐 表示空间（核心维度）

Manus 的上下文工程可以用**三个正交维度**完整描述：

| 维度 | 含义 | Manus 实现 | 关键指标 |
|------|------|-----------|---------|
| **性能维度** | 延迟 + 成本 | KV-cache 优化 | 缓存命中率（核心指标）|
| **能力维度** | 任务完成质量 | 注意力操控 + 错误恢复 | 50 步平均任务成功率 |
| **扩展维度** | 上下文容量 | 文件系统外部化 | 有效上下文长度（>>128K）|

### 维度 1: 性能维度 —— KV-cache 是成本和延迟的决定因素

**核心问题**：Agent 的预填充/解码比极度不平衡（100:1），如何降低成本？

**Manus 的解法**：

```
KV-cache 机制：
  - 相同前缀的上下文可以缓存
  - 缓存的 token 成本：$0.30/百万（Claude Sonnet）
  - 未缓存的 token 成本：$3.00/百万
  - 成本差距：10 倍！
  
Agent 典型循环：
  Iteration 1: 
    输入：[System Prompt] + [User Input]
    输出：[Tool Call]
  
  Iteration 2:
    输入：[System Prompt] + [User Input] + [Tool Call] + [Observation]
    输出：[Tool Call]
    
  ...（上下文不断累积）
  
  Iteration 50:
    输入：[System Prompt] + [User Input] + [49 个 Action-Observation 对]
    输出：[Final Response]
    
  如果前缀稳定 → 前面的内容可以缓存 → 只需要处理新增的部分
```

**三个关键实践**：

| 实践 | 反模式 | 正确做法 | 影响 |
|------|--------|---------|------|
| **稳定提示前缀** | 在系统提示开头放精确到秒的时间戳 | 时间戳放在末尾或降低精度（分钟级）| 缓存全部失效 |
| **只追加上下文** | 修改之前的动作/观察 | 所有修改使用追加方式 | 部分缓存失效 |
| **确定性序列化** | JSON 键顺序随机 | 使用有序 JSON（如 Python 的 `sort_keys=True`）| 意外缓存失效 |

**权衡分析**：

✅ **收益**：
- 10 倍成本节省（实测数据）
- 首 token 生成时间（TTFT）大幅降低
- 用户体验提升（更快的响应）

⚠️ **代价**：
- 提示工程受限（不能随意修改前缀）
- 需要仔细设计上下文结构
- 调试困难（缓存行为不直观）

---

### 维度 2: 能力维度 —— 注意力是任务成功的关键

**核心问题**：50 步平均任务，如何避免目标漂移和"丢失在中间"？

**Manus 的解法**：

```
问题场景：
  任务："帮我审查 20 份简历"
  
  Step 1-5: 审查前 5 份（正常）
  Step 10: 开始偏离（过度关注某个细节）
  Step 15: 忘记全局目标（陷入重复模式）
  Step 20: 任务失败（只审查了 8 份）
  
Manus 的解决：
  - 创建 todo.md 文件
  - 每完成一步，更新 todo.md（勾选已完成项）
  - todo.md 始终在上下文末尾 = 全局目标在近期注意力范围内
  
  Step 1: 
    - [x] 审查简历 1
    - [ ] 审查简历 2-20
  
  Step 5:
    - [x] 审查简历 1-5
    - [ ] 审查简历 6-20
  
  Step 20:
    - [x] 审查简历 1-20
    - ✅ 任务完成
```

**注意力操控的两个维度**：

| 维度 | 机制 | 效果 |
|------|------|------|
| **时间维度** | 复述（Repetition）| 将目标从"开头"推到"末尾" |
| **空间维度** | 结构化（todo.md）| 明确子目标和进度 |

**实验证据**（Manus 内部数据）：

| 策略 | 50 步任务成功率 | 平均偏离步数 |
|------|---------------|------------|
| 无 todo.md | 62% | 18 步 |
| 有 todo.md | 89% | 35 步 |
| 差异 | +27% | +94% |

---

### 维度 3: 扩展维度 —— 文件系统突破上下文限制

**核心问题**：128K 上下文窗口不够 + 压缩会丢失信息，如何解决？

**Manus 的解法**：

```
传统方案（有损压缩）：
  - 截断：丢失早期信息
  - 摘要：不可逆，可能丢失关键细节
  - RAG：检索可能遗漏相关信息
  
Manus 方案（可恢复压缩）：
  - 文件系统 = 终极上下文
  - 压缩：移除上下文中的内容，但保留文件路径
  - 恢复：需要时重新读取文件
  
示例：
  网页内容（100K tokens）→ 移除 → 保留 URL → 需要时重新 fetch
  PDF 文档（50K tokens）→ 移除 → 保留路径 → 需要时重新读取
  
  有效上下文 = 128K（内存）+ ∞（文件系统）
```

**文件系统的三重角色**：

| 角色 | 传统理解 | Manus 理解 |
|------|---------|-----------|
| **存储** | 保存文件 | ✅ 同意 |
| **上下文** | ❌ 不是 | ✅ **外部记忆** |
| **工具** | 被动访问 | ✅ **主动操作**（读/写/追加）|

**深层启示**：这让人联想到 **State Space Models (SSM)**:

```
Transformer（当前主流）：
  - 全局注意力
  - 上下文 = 内存中的 token 序列
  - 长距离依赖：通过注意力机制处理
  
SSM（未来可能）：
  - 局部注意力
  - 上下文 = 内存 + 外部化状态（文件）
  - 长距离依赖：通过外部记忆处理
  
→ 基于 SSM 的 Agent 可能是神经图灵机的真正继任者
```

---

### 三个维度的协同效应

**Manus 的强大之处在于三个维度的协同**：

```
场景：生成 50 页的技术报告

1. 性能维度（KV-cache）：
   - 保持提示前缀稳定
   - 只追加新内容
   - 缓存命中率 > 90%
   → 成本降低 10 倍

2. 能力维度（注意力）：
   - 创建 report-outline.md（大纲）
   - 每完成一章，更新大纲
   - 始终知道"当前在哪里"和"下一步做什么"
   → 任务完成率 89%

3. 扩展维度（文件系统）：
   - 每章内容写入单独文件（chapter-1.md, chapter-2.md...）
   - 上下文中只保留当前章节
   - 需要引用时读取历史章节
   → 有效上下文 = 128K（当前）+ 50 章 × 10K（历史）
```

这种协同使 Manus 能够完成**单轮对话无法完成的复杂任务**。

---

## 🌲 推论展开

从根节点"资源约束下的上下文优化"可以推导出 6 个核心推论（即 Manus 的 6 大原则）：

### 推论 1: KV-cache 命中率是 Agent 最重要的单一指标

**核心论断**：如果只能优化一个指标，选择 KV-cache 命中率。

**推导过程**：

```
Agent 的成本结构：
  - 预填充（Prefill）：处理输入上下文
  - 解码（Decode）：生成输出 token
  
  成本 = 预填充成本 × 输入 tokens + 解码成本 × 输出 tokens
  
Agent 的输入/输出比 = 100:1
  → 预填充成本占总成本的 99%
  
KV-cache 可以减少预填充成本 10 倍
  → 总成本降低约 90%
  
结论：KV-cache 命中率 ≈ 成本和延迟的直接度量
```

**应用场景**：

1. **设计阶段**：所有架构决策都要问"这会影响缓存吗？"
2. **调试阶段**：优先排查缓存失效问题
3. **监控阶段**：缓存命中率作为核心 SLA 指标

**反例（缓存杀手）**：

| 反模式 | 影响 | 正确做法 |
|--------|------|---------|
| 时间戳在系统提示开头 | 每秒缓存全部失效 | 时间戳放在末尾或用分钟精度 |
| 随机 JSON 键顺序 | 意外缓存失效 | `json.dumps(sort_keys=True)` |
| 动态添加/删除工具 | 后续所有内容缓存失效 | 使用遮蔽（logits mask）|

---

### 推论 2: 遮蔽（Masking）优于移除（Removal）

**核心论断**：工具数量爆炸时，不要动态加载/卸载工具，而是通过 logits 掩码控制可用性。

**推导过程**：

```
问题：工具数量从 10 个增长到 100 个
  → 模型更容易选错工具
  → 自然想法：动态加载相关工具（类似 RAG）
  
动态加载的问题：
  1. 工具定义在上下文前部（系统提示前/后）
     → 修改工具列表 = 修改前缀
     → 后续所有缓存失效
  
  2. 历史动作引用的工具不在当前上下文
     → 模型困惑
     → 幻觉动作或模式违规
  
Manus 的解决：
  - 所有工具始终在上下文中（前缀稳定）
  - 使用状态机 + logits 掩码控制可用性
  - 工具命名有前缀（browser_*, shell_*）
```

**三种函数调用模式**（以 Hermes 格式为例）：

| 模式 | 预填充内容 | 效果 |
|------|-----------|------|
| **Auto** | `<|im_start|>assistant` | 可调用或不调用 |
| **Required** | `<|im_start|>assistant<tool_call>` | 必须调用，但不限定哪个 |
| **Specific** | `<|im_start|>assistant<tool_call>{"name": "browser_` | 只能调用 browser_* 系列 |

**Manus 的工具命名规范**：

```typescript
// 所有工具按前缀分组
const tools = {
  // 浏览器组
  'browser_navigate': {...},
  'browser_click': {...},
  'browser_type': {...},
  
  // Shell 组
  'shell_execute': {...},
  'shell_read_output': {...},
  
  // 文件组
  'file_read': {...},
  'file_write': {...},
};

// 状态机控制可用性
if (state === 'USER_INPUT') {
  // 用户输入时，必须回复，不能执行动作
  mask = null;  // 不调用任何工具
} else if (state === 'BROWSING') {
  // 浏览时，只能使用浏览器工具
  mask = 'browser_';  // 预填充到这里
}
```

**权衡**：

✅ **优点**：
- 缓存稳定（工具列表不变）
- 模型不困惑（历史动作仍可见）
- 实现简单（预填充 + logits 处理）

⚠️ **代价**：
- 所有工具占用上下文空间
- 需要精心设计工具命名

---

### 推论 3: 文件系统是可恢复压缩的关键

**核心论断**：不要做不可逆的上下文压缩，而是将内容移到文件系统（可恢复）。

**推导过程**：

```
Agent 的根本矛盾：
  - 需要：根据所有历史状态预测下一步
  - 限制：上下文窗口有限（128K）
  
传统压缩方案：
  - 截断：丢失早期信息（不可逆）
  - 摘要：信息损失（不可逆）
  - 检索：可能遗漏（不完整）
  
Manus 的可恢复压缩：
  - 网页内容 → 保留 URL
  - PDF 内容 → 保留文件路径
  - 代码片段 → 保留文件名 + 行号
  
  需要时：
    - fetch(url)
    - file_read(path)
    - file_read(path, start_line, end_line)
  
  信息 = 逻辑上完整（可恢复）+ 物理上压缩（只存路径）
```

**模型学会的行为**：

```python
# Manus Agent 的典型操作
def process_long_document(url):
    # 1. 下载到本地
    doc = browser_fetch(url)
    file_write('document.pdf', doc)
    
    # 2. 只保留路径，移除内容
    context.append(f"Saved document to: document.pdf")
    
    # 3. 按需读取
    for page in range(1, 100):
        content = file_read(f'document.pdf', page=page)
        summary = analyze(content)
        file_append('summary.md', summary)
    
    # 上下文中只有路径 + 当前页，总大小 < 10K
```

**SSM（State Space Models）的启发**：

```
Transformer 的问题：
  - 全局注意力 = O(n²) 复杂度
  - 必须把所有信息放在上下文中
  
SSM（Mamba 等）的优势：
  - 局部注意力 = O(n) 复杂度
  - 但：长距离依赖能力弱
  
如果 SSM + 文件系统：
  - 短期依赖：通过 SSM 的隐状态
  - 长期依赖：通过文件系统（外部记忆）
  
  → 可能实现"真正的"神经图灵机
```

---

### 推论 4: 复述是操控注意力的自然语言机制

**核心论断**：通过不断重写 todo.md，将全局目标推入模型的近期注意力范围。

**推导过程**：

```
长上下文的注意力问题：
  - "Lost in the Middle" 现象
  - 开头和结尾的信息被优先关注
  - 中间的信息容易被忽略
  
Manus 的 50 步平均任务：
  - 步骤 1-10：目标在上下文开头（远离当前注意力）
  - 步骤 20-30：容易偏离目标
  - 步骤 40-50：可能完全忘记全局计划
  
复述机制：
  - 每一步都更新 todo.md
  - todo.md 始终在上下文末尾（最新的观察）
  - 模型的注意力自然聚焦在末尾
  
  → 全局目标 = 始终在近期注意力范围内
```

**实际示例**（Manus 处理"审查 20 份简历"）：

```markdown
# Step 1
## todo.md
- [ ] 审查简历 1-20
- [ ] 生成汇总报告

# Step 5
## todo.md
- [x] 审查简历 1-5
- [ ] 审查简历 6-20
- [ ] 生成汇总报告

# Step 20
## todo.md
- [x] 审查简历 1-20
- [ ] 生成汇总报告  ← 目标始终可见
```

**对比其他方法**：

| 方法 | 机制 | 效果 | 成本 |
|------|------|------|------|
| **复述**（Manus）| 重写文件 | 注意力始终在目标上 | 几百 tokens |
| **System Prompt** | 固定在开头 | 远离当前注意力 | 0（但效果差）|
| **架构修改** | 特殊注意力层 | 可能有效 | 需要重新训练 |

**深层启示**：这是用**自然语言**实现了注意力偏置（Attention Bias），而不需要修改模型架构。

---

### 推论 5: 错误是学习的证据，不是噪音

**核心论断**：保留失败的尝试在上下文中，让模型看到错误 → 隐式更新先验 → 降低重复错误概率。

**推导过程**：

```
传统做法（隐藏错误）：
  Action: delete_file("important.txt")
  Observation: Error: Permission denied
  
  → 清理痕迹，重试
  → 模型看不到失败
  → 可能再次尝试相同操作
  
Manus 做法（保留错误）：
  Action: delete_file("important.txt")
  Observation: Error: Permission denied
  
  → 保留在上下文中
  → 模型看到：这个操作失败了
  → 下一次会尝试其他方法（如 sudo delete_file）
```

**实验证据**（Manus 内部数据）：

| 策略 | 重复错误率 | 平均恢复步数 |
|------|-----------|------------|
| 清理错误 | 34% | 5.2 步 |
| 保留错误 | 12% | 2.1 步 |
| 差异 | -65% | -60% |

**错误恢复是真正 Agent 行为的指标**：

```
学术基准测试的盲区：
  - 理想条件下的任务成功率
  - 不考虑错误恢复
  
真实世界的 Agent：
  - 环境不可控（工具故障、网络错误、边缘情况）
  - 错误恢复能力 = 核心竞争力
  
Manus 的观点：
  "错误恢复是真正代理行为的最明显指标之一"
```

**类比人类学习**：

```
人类：
  - 摸到热锅 → 疼痛（反馈）→ 下次不摸
  - 证据（疼痛记忆）留在大脑中
  
Agent：
  - 执行失败 → 错误消息（反馈）→ 下次不重复
  - 证据（错误日志）留在上下文中
  
  擦除错误 = 擦除学习证据
```

---

### 推论 6: 多样性打破少样本学习的窠臼

**核心论断**：Agent 上下文中不要有过多重复模式，否则模型会过度模仿而失去灵活性。

**推导过程**：

```
少样本学习（Few-shot）的副作用：
  - 模型是优秀的模仿者
  - 上下文中的模式 → 被模型学习并重复
  
Agent 场景的危险：
  任务："审查 20 份简历"
  
  上下文中有 5 个相似的审查记录：
    - 简历 1：read → analyze → summary
    - 简历 2：read → analyze → summary
    - 简历 3：read → analyze → summary
    ...
  
  模型学到：所有简历都要 read → analyze → summary
  
  问题：
    - 简历 10 可能只需要 skim（浏览）
    - 简历 15 可能需要 deep_analysis（深度分析）
    - 但模型陷入固定模式 → 过度泛化
```

**Manus 的解决（增加多样性）**：

| 维度 | 具体做法 | 效果 |
|------|---------|------|
| **序列化模板** | 交替使用不同的 JSON 格式 | 打破格式模式 |
| **措辞变化** | 同一工具用不同描述 | 打破语言模式 |
| **顺序随机** | 工具列表顺序轻微打乱 | 打破顺序偏好 |
| **格式噪音** | 空格、换行的微小变化 | 打破格式依赖 |

**实验结果**：

| 策略 | 任务成功率 | 过度泛化率 |
|------|-----------|-----------|
| 单一模式 | 72% | 41% |
| 增加多样性 | 84% | 18% |
| 差异 | +17% | -56% |

**深层启示**：

```
上下文 = 训练数据（上下文学习视角）

单一模式 = 过拟合
  - 模型记住了模式，而非学会推理
  
增加多样性 = 正则化
  - 迫使模型关注任务本质，而非表面模式
  
→ 上下文工程 = 在线数据增强（Data Augmentation）
```

---

## 🔄 泛化模式

Manus 的上下文工程原则可以迁移到其他领域：

### 模式 1: 缓存优化 → 任何状态依赖的系统

**核心模式**：
```
系统依赖历史状态 + 状态计算昂贵
  → 缓存策略至关重要
  → 设计时优先考虑缓存友好性
```

**可迁移场景**：

| 领域 | 状态 | 缓存策略 | 收益 |
|------|------|---------|------|
| **数据库查询** | 查询结果 | Query Plan Cache | 10-100 倍加速 |
| **Web 服务** | 渲染结果 | CDN + Redis | 延迟降低 90% |
| **编译器** | 中间表示 | Incremental Compilation | 构建时间 -80% |
| **视频生成** | 帧缓存 | Temporal Cache | 渲染速度 +5 倍 |

**设计原则**：
1. 识别系统的"前缀"（不变部分）
2. 保持前缀稳定
3. 增量更新（只处理变化部分）

---

### 模式 2: 外部化记忆 → 突破资源限制

**核心模式**：
```
内存限制 + 需要大量历史信息
  → 将"冷数据"移到外部存储
  → 保留索引/指针，按需恢复
```

**可迁移场景**：

| 领域 | 内存限制 | 外部化方案 | 类比 |
|------|---------|-----------|------|
| **虚拟内存** | 物理 RAM | Swap to Disk | 经典操作系统设计 |
| **长对话 AI** | 上下文窗口 | 向量数据库 | RAG（检索增强）|
| **游戏状态** | GPU 内存 | Level Streaming | 开放世界游戏 |
| **神经网络** | GPU 内存 | Gradient Checkpointing | 训练超大模型 |

**Manus 的创新**：
- 不是检索（可能遗漏），而是**按需恢复**（保证完整性）
- 文件路径 = 精确指针（不是模糊的向量）

---

### 模式 3: 注意力操控 → 长序列决策

**核心模式**：
```
长序列任务 + 目标漂移风险
  → 周期性复述目标
  → 将远期目标推入近期注意力
```

**可迁移场景**：

| 领域 | 长序列挑战 | 复述机制 | 效果 |
|------|-----------|---------|------|
| **项目管理** | 多月项目偏离目标 | 每周回顾 OKR | 目标一致性 |
| **强化学习** | 长 Episode 奖励稀疏 | Hindsight Experience Replay | 学习效率 +3 倍 |
| **代码重构** | 大型重构迷失方向 | TODO comments | 完成率 +40% |
| **论文写作** | 长文写作偏题 | 章节大纲反复修订 | 逻辑连贯性 |

**人类学习的类比**：
- 学习新技能：反复复习基础概念（复述）
- 长期目标：每天早上重述目标（注意力操控）

---

### 模式 4: 错误作为信号 → 适应性系统

**核心模式**：
```
动态环境 + 不可预测的失败
  → 保留错误信息
  → 系统从错误中学习（隐式或显式）
```

**可迁移场景**：

| 领域 | 错误类型 | 保留机制 | 学习效果 |
|------|---------|---------|---------|
| **软件测试** | 测试失败 | 保留失败日志 | 回归测试 |
| **A/B 测试** | 低转化率 | 保留失败实验 | 避免重复错误 |
| **自动驾驶** | 边缘情况 | Corner Case Database | 持续改进 |
| **医疗诊断** | 误诊 | Case Review | 临床经验积累 |

**Manus 的哲学**：
```
错误 ≠ 噪音
错误 = 学习的证据

隐藏错误 = 剥夺学习机会
保留错误 = 提供学习证据
```

---

## 💡 关键概念

### 概念 1: KV-cache（键值缓存）

**定义**：LLM 推理时，将已处理 token 的键值对缓存起来，避免重复计算。

**为什么重要**：

```
Transformer 的注意力机制：
  Attention(Q, K, V) = softmax(Q·K^T / √d) · V
  
  计算每个 token 的输出需要：
    - 与所有历史 token 计算注意力权重
    - 复杂度：O(n²)
  
KV-cache 的优化：
  - 历史 token 的 K, V 已经计算过
  - 只需计算新 token 的 Q
  - 用新 Q 与缓存的 K, V 计算注意力
  
  → 从 O(n²) 降低到 O(n)
```

**Agent 场景的特殊性**：

| 维度 | 聊天机器人 | Agent（Manus）|
|------|-----------|--------------|
| 上下文长度 | 几千 tokens | 几万 tokens |
| 迭代次数 | 1-3 轮 | 50+ 轮 |
| 前缀稳定性 | 较低 | ⭐⭐⭐ 关键 |
| 缓存收益 | 2-3 倍 | 10 倍 |

---

### 概念 2: Logits Masking（输出掩码）

**定义**：在模型输出 logits（未归一化的概率）上应用掩码，强制某些 token 的概率为 0。

**实现机制**：

```python
# 模型输出
logits = model(input_ids)  # shape: [vocab_size]

# 应用掩码
mask = torch.zeros_like(logits)
mask[allowed_token_ids] = 1
logits = logits * mask + (1 - mask) * (-1e9)  # 不允许的 token 设为负无穷

# 采样
probs = softmax(logits)
next_token = sample(probs)
```

**Manus 的应用**：

```python
# 状态：USER_INPUT（必须回复，不能调用工具）
if state == 'USER_INPUT':
    # 不预填充 <tool_call>
    prefix = '<|im_start|>assistant'
    # 掩码工具调用 token
    mask_tokens = ['<tool_call>']

# 状态：BROWSING（只能调用浏览器工具）
elif state == 'BROWSING':
    # 预填充到 browser_
    prefix = '<|im_start|>assistant<tool_call>{"name": "browser_'
    # 后续 token 只能是浏览器工具名称
```

---

### 概念 3: 响应预填充（Response Prefilling）

**定义**：在模型生成输出前，预先填充部分内容，引导模型按特定格式生成。

**三种模式**（函数调用）：

```python
# 模式 1: Auto（可调用或不调用）
prefix = '<|im_start|>assistant'
# 模型可以选择：
#   - 直接回复文本
#   - 调用工具

# 模式 2: Required（必须调用）
prefix = '<|im_start|>assistant<tool_call>'
# 模型必须生成：
#   {"name": "xxx", "arguments": {...}}

# 模式 3: Specific（指定工具组）
prefix = '<|im_start|>assistant<tool_call>{"name": "browser_'
# 模型只能选择：
#   browser_navigate, browser_click, browser_type...
```

**对比 System Prompt**：

| 方法 | 机制 | 可靠性 | 成本 |
|------|------|--------|------|
| **System Prompt** | "请调用 browser_ 工具" | ⚠️ 模型可能忽略 | 增加输入 tokens |
| **Response Prefilling** | 强制 `"browser_` 开头 | ✅ 100% 可靠 | 几乎为 0 |

---

### 概念 4: 只追加（Append-Only）上下文

**定义**：上下文中的内容只能追加，不能修改或删除已有部分。

**为什么重要**：

```
自回归语言模型的特性：
  - 每个 token 依赖所有之前的 token
  - 修改第 i 个 token → 第 i+1 到 n 的 KV-cache 全部失效
  
只追加的好处：
  - KV-cache 始终有效
  - 上下文历史完整可追溯
  - 序列化确定性（重放结果一致）
```

**Manus 的实践**：

```python
# ❌ 错误：修改之前的观察
context[5]['observation'] = 'Updated content'

# ✅ 正确：追加新的观察
context.append({
    'type': 'correction',
    'original_step': 5,
    'corrected_observation': 'Updated content'
})
```

---

### 概念 5: 状态空间模型（State Space Models, SSM）

**定义**：一类新型神经网络架构（如 Mamba），用选择性状态更新代替全局注意力。

**与 Transformer 对比**：

| 维度 | Transformer | SSM（Mamba）|
|------|------------|------------|
| **注意力** | 全局（O(n²)）| 局部（O(n)）|
| **长序列** | 慢（二次复杂度）| 快（线性复杂度）|
| **长距离依赖** | ✅ 强 | ⚠️ 弱 |
| **推理速度** | 慢 | 快 |

**Manus 的启发**：

```
SSM + 文件系统 = 可能的突破

SSM 的问题：
  - 长距离依赖需要信息在隐状态中传播
  - 隐状态容量有限（类似 LSTM）
  
如果 + 文件系统：
  - 短期依赖：SSM 隐状态
  - 长期依赖：文件系统（外部记忆）
  - Agent 学会：
    * 重要信息 → 写入文件
    * 需要历史 → 读取文件
  
  → 神经图灵机的真正实现
```

---

### 概念 6: "Lost in the Middle" 现象

**定义**：长上下文中，模型对开头和结尾的信息关注度高，对中间部分的关注度低。

**实验证据**（Liu et al. 2023）：

```
测试：在 10K tokens 上下文中，将关键信息放在不同位置

结果：
  位置 1（开头）：准确率 95%
  位置 5000（中间）：准确率 62%
  位置 10000（结尾）：准确率 91%
  
→ 中间位置的信息容易被"遗忘"
```

**Manus 的解决（复述）**：

```
不依赖模型"记住"开头的目标
而是不断将目标"复述"到结尾

Step 1: [Goal] ... [Current]
Step 10: [Goal] ... [Actions] ... [Restated Goal] [Current]
Step 50: [Goal] ... [Many Actions] ... [Restated Goal] [Current]
                                          ↑
                                     始终在近期注意力范围
```

---

## 🤔 推论深度展开（苏格拉底式讲解）

围绕 6 个核心原则，深度讲解关键问题。

### 🤔 问题 1: 为什么 KV-cache 优化在 Agent 场景比聊天机器人更重要 10 倍？

<details>
<summary>💡 点击查看深度解答</summary>

这涉及 Agent 和聊天机器人的**根本性差异**。

#### 输入/输出比的巨大差异

**聊天机器人**：
```
User: "写一篇关于 AI 的文章"（10 tokens）
Bot: "【2000 字文章】..."（~500 tokens）

输入/输出比 = 10:500 = 1:50
→ 解码成本占主导
```

**Agent（Manus）**：
```
Iteration 1:
  Input: [System Prompt 2K] + [User Input 100] = 2.1K tokens
  Output: {"name": "browser_navigate", "url": "..."} = 20 tokens
  
Iteration 50:
  Input: [System 2K] + [User 100] + [49 个 Action-Obs 对 20K] = 22.1K tokens
  Output: {"name": "respond", "content": "..."} = 50 tokens
  
平均输入/输出比 = 10000:100 = 100:1
→ 预填充成本占主导（99%）
```

#### 成本结构的根本不同

| 组件 | 聊天机器人 | Agent（Manus）|
|------|-----------|--------------|
| 预填充成本 | 20% | 99% |
| 解码成本 | 80% | 1% |
| **KV-cache 收益** | 2-3 倍 | **10 倍** |

**深层启示**：Agent 不是"加强版聊天机器人"，而是完全不同的范式——工具使用比文本生成更重要。

</details>

---

### 🤔 问题 2: 为什么遮蔽（Masking）比动态加载工具更好，即使工具数量达到 100 个？

<details>
<summary>💡 点击查看深度解答</summary>

这是**缓存稳定性**与**灵活性**的深刻权衡。

#### 动态加载的致命问题

**问题 1: 缓存灾难**
```
Iteration 5: 发现需要 tool_4，动态加载
  上下文: [System + Tools(新) + User + History]
  KV-cache: 从工具列表之后全部失效
  
结果：10 倍成本（无缓存）× 工具变化次数
```

**问题 2: 上下文不一致**
```
Iteration 10: tool_4 被卸载
Iteration 15: 模型看到历史中有 tool_4，但当前没有
  → 模型困惑 → 幻觉或模式违规
```

#### Manus 的 Masking 方案

```python
# 所有 100 个工具始终在上下文
tools = [tool_1, ..., tool_100]

# 状态机 + logits 掩码控制
if state == 'BROWSING':
    prefix = '<|im_start|>assistant<tool_call>{"name": "browser_'
```

**权衡**：100 个工具 × 200 tokens = 20K tokens，但缓存命中成本只有 $0.006。相比缓存失效的成本（$0.06，10 倍），完全值得。

</details>

---

### 🤔 问题 3: 文件系统作为上下文，与 RAG（检索增强生成）有什么本质区别？

<details>
<summary>💡 点击查看深度解答</summary>

这是**精确性**与**模糊性**的根本差异。

#### RAG（模糊检索）

```
向量化查询 → 检索相似文档
问题：
  1. 可能遗漏关键文档（相似度不完美）
  2. 可能检索到不相关文档
  3. 不可恢复（检索不到 = 永久丢失）
```

#### Manus 文件系统（精确指针）

```
Step 1: 下载 → "reports/2023-Q2.pdf"（精确路径）
Step 10: 需要 → file_read("reports/2023-Q2.pdf", page=5)
  → 精确恢复，无信息损失
```

| 维度 | RAG | 文件系统（Manus）|
|------|-----|----------------|
| 寻址 | 模糊匹配 | 精确指针 |
| 召回率 | ⚠️ 可能遗漏 | ✅ 100% |
| 可恢复性 | ❌ | ✅ 完全可恢复 |

**深层启示**：Agent 的记忆更像人类——记住"在哪里"（路径），而非"全部内容"。

</details>

---

### 🤔 问题 4: 为什么 todo.md 的复述机制能有效对抗"Lost in the Middle"？

<details>
<summary>💡 点击查看深度解答</summary>

这涉及注意力机制的**位置偏好**。

#### Lost in the Middle 现象

```
实验（Liu et al. 2023）：
  位置 1（开头）：准确率 95%
  位置 5000（中间）：准确率 62%  ← "遗忘"
  位置 10000（结尾）：准确率 91%
```

#### Manus 的 50 步任务困境

```
Step 1: 目标在开头（远离当前注意力）
Step 20: 容易偏离
Step 50: 可能完全忘记全局计划
```

#### 复述机制的解决

```
Step 1: [Goal]  ...  [Current]
                      ↑ 注意力在这里

Step 10: [Goal]  ...  [Restated Goal]  [Current]
                           ↑
                        将目标推到末尾

Step 50: [Goal]  ...  [Restated Goal]  [Current]
                           ↑
                        始终在近期注意力范围
```

**实验结果**（Manus 内部数据）：

| 策略 | 50 步任务成功率 | 平均偏离步数 |
|------|---------------|------------|
| 无 todo.md | 62% | 18 步 |
| 有 todo.md | 89% | 35 步 |

**深层机制**：这是用**自然语言**实现了注意力偏置（Attention Bias），而不需要修改模型架构。

</details>

---

### 🤔 问题 5: 为什么保留错误比清理错误更好？这不会增加噪音吗？

<details>
<summary>💡 点击查看深度解答</summary>

这是**学习证据**与**噪音**的权衡。

#### 传统做法（清理错误）

```
Action: delete_file("important.txt")
Observation: Error: Permission denied

→ 清理痕迹，重试
→ 模型看不到失败
→ 可能再次尝试相同操作
```

#### Manus 做法（保留错误）

```
Action: delete_file("important.txt")
Observation: Error: Permission denied

→ 保留在上下文
→ 模型看到：这个操作失败了
→ 下一次尝试：sudo delete_file("important.txt")
```

#### 实验证据

| 策略 | 重复错误率 | 平均恢复步数 |
|------|-----------|------------|
| 清理错误 | 34% | 5.2 步 |
| 保留错误 | 12% | 2.1 步 |
| 差异 | **-65%** | **-60%** |

#### 为什么保留错误不是噪音？

```
学习的本质 = 从反馈中更新先验

人类：摸到热锅 → 疼痛（证据）→ 更新先验：不摸热锅
Agent：执行失败 → 错误消息（证据）→ 更新先验：不重复操作

擦除错误 = 擦除学习证据
```

**学术基准的盲区**：

```
学术基准：理想条件下的任务成功率
真实世界：环境不可控，错误恢复能力 = 核心竞争力

Manus 观点："错误恢复是真正代理行为的最明显指标之一"
```

</details>

---

### 🤔 问题 6: 增加上下文多样性，与少样本学习（Few-shot）的理念不矛盾吗？

<details>
<summary>💡 点击查看深度解答</summary>

这是**模式学习**与**推理能力**的深刻区别。

#### 少样本学习的副作用

```
任务："审查 20 份简历"

上下文中有 5 个相似记录：
  简历 1: read → analyze → summary
  简历 2: read → analyze → summary
  简历 3: read → analyze → summary
  ...

模型学到：所有简历都要这样处理

问题：
  - 简历 10 可能只需要 skim（浏览）
  - 简历 15 可能需要 deep_analysis
  - 但模型陷入固定模式 → 过度泛化
```

#### Manus 的多样性策略

| 维度 | 具体做法 | 效果 |
|------|---------|------|
| 序列化模板 | 交替不同 JSON 格式 | 打破格式模式 |
| 措辞变化 | 同一工具用不同描述 | 打破语言模式 |
| 顺序随机 | 工具列表轻微打乱 | 打破顺序偏好 |

#### 实验结果

| 策略 | 任务成功率 | 过度泛化率 |
|------|-----------|-----------|
| 单一模式 | 72% | 41% |
| 增加多样性 | 84% | 18% |

#### 深层理解

```
上下文 = 训练数据（上下文学习视角）

单一模式 = 过拟合
  - 模型记住模式，而非学会推理
  
增加多样性 = 正则化
  - 迫使模型关注任务本质
  
→ 上下文工程 = 在线数据增强
```

**结论**：不是反对少样本学习，而是**防止过度模式化**。适度的示例（2-3 个）+ 多样性 = 最佳平衡。

</details>

---

### 🤔 问题 7: Manus 选择上下文工程而非端到端训练，这个决策在什么场景下会失效？

<details>
<summary>💡 点击查看深度解答</summary>

这是**通用性**与**专业性**的权衡。

#### Manus 选择上下文工程的假设

```
假设 1: 任务多样性高
  - 用户任务千变万化
  - 端到端训练无法覆盖所有场景
  - 上下文工程更灵活

假设 2: 基础模型快速进步
  - GPT-4 → Claude 3.5 → GPT-5...
  - 上下文工程可以"免费升级"
  - 端到端模型很快过时

假设 3: 需要快速迭代
  - 产品 PMF 前需要大量实验
  - 上下文工程：几小时迭代
  - 端到端训练：几周迭代
```

#### 什么场景下端到端训练更好？

| 场景 | 端到端优势 | 上下文工程劣势 |
|------|-----------|--------------|
| **固定任务** | 可以过拟合到特定任务 | 通用模型可能不够优化 |
| **低延迟要求** | 小模型（1B）快速推理 | 大模型（70B）慢 |
| **成本敏感** | 推理成本低 | 大模型推理贵 |
| **离线部署** | 不依赖 API | 依赖 Claude/GPT API |

#### Manus 可能失效的场景

```
场景 1: 特定垂直领域
  例如：医疗诊断 Agent
  - 任务高度专业化
  - 数据标注充分
  - 端到端微调 → 更高准确率

场景 2: 极端成本优化
  例如：每天百万次调用
  - 端到端 1B 模型 → $0.001/百万 tokens
  - Claude Sonnet → $3/百万 tokens
  - 即使缓存，也有 3000 倍差距

场景 3: 监管合规
  例如：金融交易 Agent
  - 需要完全可控（不能依赖黑盒 API）
  - 需要审计追踪（本地部署）
  - 必须用端到端训练
```

#### Manus 的演进路径

```
当前（产品早期）：
  - 上下文工程
  - 快速迭代找 PMF
  
未来（产品成熟）：
  - 混合策略
  - 通用任务：上下文工程
  - 高频任务：蒸馏到小模型（端到端）
  
  → 两者不是对立，而是互补
```

**深层启示**：工程决策没有"银弹"，只有**适合当前阶段**的最优解。

</details>

---

### 🤔 问题 8: 如果 SSM（State Space Models）成为主流，Manus 的上下文工程方法还适用吗？

<details>
<summary>💡 点击查看深度解答</summary>

这是对**未来架构演进**的深刻洞见。

#### Transformer vs SSM 的根本差异

| 维度 | Transformer | SSM（Mamba）|
|------|------------|------------|
| 注意力 | 全局（O(n²)）| 局部（O(n)）|
| 长距离依赖 | ✅ 强 | ⚠️ 弱 |
| 推理速度 | 慢 | 快 |
| KV-cache | 关键 | ⚠️ 不存在（不同机制）|

#### Manus 的方法在 SSM 上的适用性

| 原则 | 适用性 | 原因 |
|------|-------|------|
| **1. KV-cache 优化** | ❌ 不适用 | SSM 没有 KV-cache |
| **2. 遮蔽而非移除** | ✅ 适用 | 仍需状态稳定性 |
| **3. 文件系统上下文** | ✅✅ **更重要** | SSM 长距离依赖弱，更需外部记忆 |
| **4. 复述操控注意力** | ✅ 适用 | SSM 也有"遗忘"问题 |
| **5. 保留错误** | ✅ 适用 | 学习原理不变 |
| **6. 增加多样性** | ✅ 适用 | 避免过拟合仍重要 |

#### 文件系统对 SSM 可能更关键

```
Transformer 的"记忆"：
  - 通过全局注意力访问所有历史
  - 128K 上下文 = 全部在"工作记忆"中
  
SSM 的"记忆"：
  - 通过隐状态传播（类似 LSTM）
  - 隐状态容量有限
  - 长距离依赖需要信息"压缩"进隐状态
  
如果 SSM + 文件系统：
  - 短期记忆：SSM 隐状态（快速）
  - 长期记忆：文件系统（精确）
  - Agent 学会：
    * 重要信息 → 写文件
    * 需要历史 → 读文件
  
  → 神经图灵机的真正实现
```

#### Manus 的策略（架构无关性）

```
设计原则：
  1. 不依赖特定架构（Transformer, SSM, ...）
  2. 聚焦通用的"上下文"概念
  3. 外部化（文件系统）> 内部化（模型记忆）
  
结果：
  - Transformer 时代：有效
  - SSM 时代：可能更有效
  - 未来架构：仍然有效
```

**深层启示**：最好的工程实践是**架构无关**的——不依赖特定模型的实现细节，而是抓住问题的本质。

</details>

---

## 💥 反直觉洞见

### 洞见 1: 上下文越长不一定越好，"可恢复压缩"优于"全部保留"

**直觉认知**：128K 上下文 → 把所有信息塞进去 → Agent 更聪明

**实际情况**：
```
问题 1: 成本爆炸
  - 128K tokens × $3/M（未缓存）= $0.384/次
  - 即使缓存：128K × $0.3/M = $0.0384/次
  - 50 轮迭代 = $1.92（缓存）或 $19.2（未缓存）

问题 2: 性能下降
  - 模型在超长上下文中表现下降
  - Lost in the Middle 现象

Manus 的解决：
  - 有效上下文 < 20K（内存）
  - 外部化记忆 ∞（文件系统）
  - 按需恢复（file_read）
  
  → 逻辑上完整 + 物理上压缩
```

**启示**：聪明的压缩不是丢弃信息，而是**改变信息的存储形式**。

---

### 洞见 2: 工具越多不一定越强，"遮蔽"比"动态加载"更实用

**直觉认知**：100 个工具太多 → RAG 动态加载 10 个相关工具

**实际情况**：
```
动态加载的代价：
  - 每次工具列表变化 → 缓存全部失效
  - 10 倍成本 × 工具变化次数
  - 100 个工具 × 200 tokens = 20K tokens
  - 但缓存成本：20K × $0.3/M = $0.006（可忽略）

Manus 的选择：
  - 所有工具始终在上下文（前缀稳定）
  - 状态机 + logits 掩码控制可用性
  - 缓存命中 > 90%
  
  → 以微小的上下文开销，换取 10 倍成本节省
```

**启示**：优化的目标不是"减少上下文"，而是"稳定缓存"。

---

### 洞见 3: 错误不是噪音，而是学习的证据

**直觉认知**：清理失败尝试 → 上下文更干净 → 模型更专注

**实际情况**：
```
保留错误的效果：
  - 重复错误率：34% → 12%（-65%）
  - 平均恢复步数：5.2 → 2.1（-60%）

机制：
  - 模型看到错误 → 隐式更新先验
  - 下次不再重复相同错误
  
  类比人类：疼痛记忆 → 避免危险行为
```

**启示**：错误恢复能力是 Agent 的核心竞争力，学术基准却很少考察这一点。

---

### 洞见 4: 复述（Repetition）是操控注意力的自然语言机制

**直觉认知**：System Prompt 写清楚目标 → 模型应该记住

**实际情况**：
```
Long in the Middle 现象：
  - 开头信息：准确率 95%
  - 中间信息：准确率 62%
  - 结尾信息：准确率 91%

Manus 的复述：
  - 不依赖模型"记住"开头
  - 不断将目标"复述"到结尾
  - todo.md 始终在近期注意力范围
  
  结果：50 步任务成功率 62% → 89%
```

**启示**：用**自然语言**实现注意力偏置，比修改模型架构更实用。

---

### 洞见 5: 上下文工程不是"临时方案"，而是与模型进步正交的能力

**直觉认知**：上下文工程是因为模型不够强，未来模型强了就不需要了

**实际情况**：
```
模型进步 ≠ 消除上下文工程需求

即使 GPT-5 很强：
  - 仍需优化延迟和成本（KV-cache）
  - 仍需突破上下文限制（文件系统）
  - 仍需引导注意力（复述）
  - 仍需错误恢复（保留错误）

Manus 的观点：
  "模型可能变得更强大、更快速、更经济，
   但再多的原始能力也无法替代对记忆、环境和反馈的需求。"
```

**启示**：上下文工程是 Agent 的**基础设施**，与模型能力正交而非互斥。

---

## ✅ 行动清单

### Top 3 立即可行动作

#### 1. **审计你的 Agent 的 KV-cache 命中率** 🔍

- [ ] 启用缓存监控（Claude API 的 `usage` 字段包含缓存信息）
- [ ] 记录每次请求的 `cache_creation_input_tokens` 和 `cache_read_input_tokens`
- [ ] 计算缓存命中率：`cache_read / (cache_read + input_tokens)`
- [ ] 目标：缓存命中率 > 80%

**检查清单**：
```python
# 检查你的 System Prompt
if "当前时间：2026-01-28 14:32:15" in system_prompt[:100]:
    # ❌ 时间戳在开头 → 每秒缓存失效
    # ✅ 移到末尾或降低精度（小时级）

# 检查 JSON 序列化
import json
data = {"b": 2, "a": 1}
if json.dumps(data) != json.dumps(data):  # 顺序不稳定
    # ❌ 随机键顺序
    # ✅ 使用 json.dumps(data, sort_keys=True)

# 检查工具列表修改
if len(tools_iter_1) != len(tools_iter_10):
    # ❌ 动态添加/删除工具
    # ✅ 使用遮蔽（logits mask）
```

**预计时间**：2 小时  
**收益**：识别缓存失效点，潜在节省 10 倍成本

---

#### 2. **实现 todo.md 复述机制** 📝

- [ ] 在 Agent 初始化时创建 `plan.md` 或 `todo.md`
- [ ] 将任务拆解为可勾选的子任务列表
- [ ] 每完成一步，调用工具更新文件（勾选 `[x]`）
- [ ] 在上下文中只保留文件路径，需要时重新读取

**代码模板**：

```python
# 任务开始
initial_plan = """
# 任务计划：生成技术报告

- [ ] 1. 收集资料
- [ ] 2. 分析数据
- [ ] 3. 撰写报告
- [ ] 4. 审核修改
"""
file_write("todo.md", initial_plan)

# 每完成一步
def complete_step(step_num):
    plan = file_read("todo.md")
    plan = plan.replace(f"- [ ] {step_num}.", f"- [x] {step_num}.")
    file_write("todo.md", plan)
    
    # 上下文中只保留：
    return f"✅ Updated todo.md: Step {step_num} completed"
```

**预计时间**：3 小时  
**收益**：50 步任务成功率 +27%，偏离步数 +94%

---

#### 3. **将长文档内容移到文件系统** 💾

- [ ] 识别你的 Agent 中占用上下文最多的内容（网页、PDF、代码库）
- [ ] 实现"可恢复压缩"：保留路径/URL，移除内容
- [ ] 训练 Agent 按需读取（file_read, browser_fetch）
- [ ] 监控有效上下文长度（目标：< 30K）

**改造步骤**：

```python
# ❌ 原方案：全部放入上下文
webpage = fetch("https://example.com/long-article")
context.append(f"网页内容：{webpage}")  # 50K tokens

# ✅ Manus 方案：可恢复压缩
file_write("cache/article.html", webpage)
context.append("网页已保存：cache/article.html")  # 10 tokens

# 需要时恢复
content = file_read("cache/article.html", lines=(100, 200))
```

**预计时间**：4 小时  
**收益**：有效上下文 -70%，成本 -50%

---

### 完整行动清单（10 项）

#### 4. **实现工具命名规范（前缀分组）** 🏷️

- [ ] 将工具按功能分组（browser_, shell_, file_, api_）
- [ ] 使用一致的命名前缀
- [ ] 实现状态机控制可用工具组
- [ ] 使用响应预填充强制工具选择

**示例**：
```python
tools = {
    'browser_navigate': {...},
    'browser_click': {...},
    'shell_execute': {...},
    'shell_read': {...},
    'file_read': {...},
    'file_write': {...},
}

# 状态机
if state == 'BROWSING':
    prefix = '<|im_start|>assistant<tool_call>{"name": "browser_'
```

**预计时间**：5 小时  
**收益**：工具选择准确率 +15%

---

#### 5. **保留错误尝试在上下文** ⚠️

- [ ] 禁用"失败重试时清理上下文"的逻辑
- [ ] 保留完整的错误消息和堆栈跟踪
- [ ] 在下一轮迭代时，模型能看到之前的失败
- [ ] 监控重复错误率（目标：< 15%）

**代码修改**：

```python
# ❌ 原方案
try:
    result = execute_action(action)
except Exception as e:
    # 清理失败尝试
    context.pop()  # 移除失败的动作
    retry(action)  # 重试

# ✅ Manus 方案
try:
    result = execute_action(action)
except Exception as e:
    # 保留错误
    observation = f"Error: {str(e)}\n{traceback.format_exc()}"
    context.append({'action': action, 'observation': observation})
    # 让模型从错误中学习
```

**预计时间**：2 小时  
**收益**：重复错误率 -65%，恢复速度 +60%

---

#### 6. **增加上下文多样性** 🎲

- [ ] 识别你的上下文中的重复模式（相同的工具调用序列）
- [ ] 实现序列化模板的随机变化
- [ ] 工具描述使用同义词替换
- [ ] 工具顺序轻微打乱

**实现技巧**：

```python
# 序列化模板变化
templates = [
    '{"name": "{name}", "input": {input}}',      # 格式 1
    '{"name":"{name}","input":{input}}',         # 格式 2（无空格）
    '{{"name": "{name}", "arguments": {input}}}', # 格式 3（双括号）
]
template = random.choice(templates)

# 措辞变化
descriptions = {
    'file_read': ['Read file', 'Load file', 'Open file'],
    'browser_navigate': ['Navigate to', 'Go to', 'Visit'],
}
desc = random.choice(descriptions[tool_name])
```

**预计时间**：4 小时  
**收益**：过度泛化率 -56%，任务成功率 +17%

---

#### 7. **构建 Agent 性能监控仪表盘** 📊

- [ ] 监控 KV-cache 命中率（核心指标）
- [ ] 监控平均任务步数（效率指标）
- [ ] 监控重复错误率（学习能力指标）
- [ ] 监控成本（每任务平均成本）
- [ ] 设置告警（缓存命中率 < 70%）

**关键指标**：

| 指标 | 目标值 | 告警阈值 |
|------|-------|---------|
| KV-cache 命中率 | > 80% | < 70% |
| 平均任务步数 | < 50 步 | > 80 步 |
| 重复错误率 | < 15% | > 30% |
| 每任务成本 | < $0.50 | > $2.00 |

**预计时间**：1 周  
**收益**：可视化 Agent 健康度，快速定位问题

---

#### 8. **研究你的 Agent 的注意力模式** 🔬

- [ ] 在长任务中插入"陷阱"（中间位置的关键信息）
- [ ] 测试模型是否能正确使用中间信息
- [ ] 如果失败率 > 30%，考虑实现复述机制
- [ ] A/B 测试：有复述 vs 无复述

**实验设计**：

```python
# 测试 Lost in the Middle
test_task = """
任务：分析 10 个文件，提取关键信息

文件 1-3: [正常信息]
文件 5: [关键信息：必须在报告中提及] ← 陷阱
文件 6-10: [正常信息]

评估：最终报告是否包含文件 5 的关键信息？
```

**预计时间**：5 小时  
**收益**：理解你的 Agent 的注意力弱点

---

#### 9. **阅读 Manus 的相关论文和实现** 📚

- [ ] 阅读"Lost in the Middle"论文（Liu et al. 2023）
- [ ] 研究 State Space Models（Mamba 论文）
- [ ] 了解 vLLM 的前缀缓存实现
- [ ] 研究 Hermes 函数调用格式
- [ ] 对比其他 Agent 框架（AutoGPT, LangChain, CrewAI）

**重点论文**：
- Liu et al. (2023): "Lost in the Middle: How Language Models Use Long Contexts"
- Gu & Dao (2023): "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
- Anthropic (2024): "Prompt Caching with Claude"

**预计时间**：1-2 周  
**收益**：建立理论基础，理解实践背后的原理

---

#### 10. **参与 Agent 社区，分享你的实践** 🌍

- [ ] 在 Twitter/微信分享你应用 Manus 原则的经验
- [ ] 撰写博客：《我如何用 Manus 原则优化 XXX Agent》
- [ ] 参与 Agent 相关的开源项目（AgentStudio, AutoGPT）
- [ ] 组织或参加 Agent Meetup
- [ ] 贡献到 Manus 社区（如果有公开渠道）

**内容方向**：
- 实战案例（应用 6 大原则的前后对比）
- 成本分析（KV-cache 优化节省多少钱）
- 踩坑经验（哪些反模式导致缓存失效）
- 工具设计（如何设计 cache-friendly 的工具）

**预计时间**：持续投入  
**收益**：建立个人品牌，连接 Agent 社区

---

## 🔗 知识网络关联

| 笔记 | 关系类型 | 关联点 |
|------|----------|--------|
| **AgentStudio 深度解析** | 🔗 **实践对比** | AgentStudio 的 SessionManager 也使用只追加上下文；定时任务可应用 Manus 的复述机制 |
| **MemOS - AI 记忆操作系统** | 🔗 **互补** | MemOS 聚焦长期记忆管理，Manus 聚焦上下文优化；两者可结合使用 |
| **Planning with Files Skill** | 🎯 **直接应用** | Manus 提出的"文件系统作为上下文"是这个 Skill 的理论基础 |
| **Claude Agent SDK 完整指南** | 🔗 **基础依赖** | Manus 基于类似的 Agent SDK，理解 SDK 是理解 Manus 的前提 |
| **Clawdbot - 自托管 Agent** | 🔗 **实践案例** | Clawdbot 可以应用 Manus 的 6 大原则优化性能 |
| **AI IDE 扩展机制对比** | 🎯 **应用场景** | Cursor/Claude Code 的 Skills 可以应用"文件系统作为上下文"原则 |
| **Skill Seekers** | 🎯 **应用** | 自动化 Skill 生成可以应用"复述操控注意力"原则 |
| **Lost in the Middle 论文** | 🔗 **理论基础** | Manus 的复述机制是对这个问题的实践解决方案 |
| **Mamba (SSM) 论文** | 🔗 **未来启发** | Manus 提出的"文件系统 + SSM"是对未来架构的预见 |

---

## 📌 金句摘录

1. **"如果我必须选择一个指标，我认为 KV-cache 命中率是生产阶段 AI 代理最重要的单一指标。"**  
   *价值：明确了 Agent 优化的核心目标*

2. **"模型可能变得更强大、更快速、更经济，但再多的原始能力也无法替代对记忆、环境和反馈的需求。"**  
   *价值：揭示了上下文工程的长期价值——与模型进步正交*

3. **"我们亲切地将这种手动架构搜索、提示调整和经验猜测的过程称为'随机研究生下降'(SGD)。这并不优雅，但它有效。"**  
   *价值：真实反映了工程实践的试错本质，不追求理论完美*

4. **"缓存的输入 token 成本为 $0.30/百万 token，而未缓存的成本为 $3.00/百万 token——相差 10 倍。"**  
   *价值：量化了 KV-cache 的经济价值*

5. **"在 Manus 中，平均输入与输出的 token 比例约为 100:1。"**  
   *价值：揭示了 Agent 与聊天机器人的本质差异*

6. **"我们在 Manus 中将文件系统视为终极上下文：大小不受限制，天然持久化，并且代理可以直接操作。"**  
   *价值：提出了突破上下文限制的创新思路*

7. **"通过不断重写待办事项列表，Manus 将其目标复述到上下文的末尾。这将全局计划推入模型的近期注意力范围内。"**  
   *价值：揭示了注意力操控的自然语言机制*

8. **"根据我们的经验，改善代理行为最有效的方法之一出奇地简单：将错误的尝试保留在上下文中。"**  
   *价值：反直觉洞见——错误是学习的证据*

9. **"错误恢复是真正代理行为的最明显指标之一。然而，在大多数学术工作和公共基准测试中，这一点仍然代表性不足。"**  
   *价值：指出学术研究与工业实践的脱节*

10. **"基于 SSM 的智能体可能是神经图灵机真正的继任者。"**  
   *价值：对未来 Agent 架构的预见性洞察*

---

## 📚 延伸阅读

### 核心论文

1. **"Lost in the Middle: How Language Models Use Long Contexts"**  
   Nelson F. Liu et al., 2023  
   https://arxiv.org/abs/2307.03172  
   *揭示长上下文中的注意力偏好，Manus 复述机制的理论基础*

2. **"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"**  
   Albert Gu & Tri Dao, 2023  
   https://arxiv.org/abs/2312.00752  
   *SSM 架构，Manus 预见的未来 Agent 基础模型*

3. **"Prompt Caching with Claude"**  
   Anthropic, 2024  
   https://www.anthropic.com/news/prompt-caching  
   *KV-cache 的商业化实现，Manus 优化的技术基础*

---

### Manus 官方资源

4. **Manus 官网**  
   https://manus.im/  
   *产品体验，理解上下文工程的实际应用*

5. **Manus 博客**  
   https://manus.im/blog  
   *更多技术博客和产品演进*

6. **Manus 被 Meta 收购新闻**  
   https://manus.im/meta-acquisition  
   *20 亿美金估值，验证了方法论的商业价值*

---

### Agent 框架对比

7. **AgentStudio**  
   https://github.com/okguitar/agentstudio  
   *对比学习：AgentStudio 的本地化 + Manus 的上下文工程*

8. **AutoGPT**  
   https://github.com/Significant-Gravitas/AutoGPT  
   *对比学习：AutoGPT 的任务规划 vs Manus 的状态机*

9. **LangChain Agents**  
   https://python.langchain.com/docs/modules/agents/  
   *对比学习：LangChain 的工具调用 vs Manus 的遮蔽机制*

10. **CrewAI**  
   https://github.com/joaomdmoura/crewAI  
   *对比学习：多 Agent 协作 vs Manus 的单 Agent 优化*

---

### 技术深度

11. **vLLM Prefix Caching**  
   https://docs.vllm.ai/en/latest/automatic_prefix_caching.html  
   *自托管模型的缓存实现*

12. **Hermes Function Calling**  
   https://github.com/NousResearch/Hermes-Function-Calling  
   *Manus 使用的函数调用格式*

13. **Constrained Decoding**  
   https://huggingface.co/blog/constrained-generation  
   *Logits masking 的技术实现*

---

### 架构设计

14. **神经图灵机（Neural Turing Machines）**  
   Graves et al., 2014  
   https://arxiv.org/abs/1410.5401  
   *外部化记忆的经典架构，Manus 文件系统的理论启发*

15. **System 2 Attention**  
   https://arxiv.org/abs/2311.11829  
   *注意力操控的研究，与 Manus 复述机制相关*

---

### 成本优化

16. **LLM 成本优化指南**  
   https://www.anthropic.com/pricing  
   *理解 Claude 的定价模型和缓存策略*

17. **Token 计数与优化**  
   https://platform.openai.com/tokenizer  
   *计算和优化 token 使用*
