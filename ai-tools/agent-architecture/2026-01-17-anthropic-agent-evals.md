---
title: "Anthropic 揭秘 AI Agents 评估"
date: 2026-01-17
category: ai-tools/agent-architecture
tags: [agent, evaluation, benchmark, anthropic, best-practice]
source: https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents
---

# Anthropic 揭秘 AI Agents 评估 - 官方最佳实践指南

> **原文**：[Demystifying Evals for AI Agents](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents)
> **作者**：Anthropic 工程团队
> **精读日期**：2026-01-17

## 📖 快速概览

| 项目 | 内容 |
|------|------|
| **标题** | Demystifying Evals for AI Agents |
| **作者** | Anthropic 工程团队 |
| **类型** | 技术博客（官方最佳实践） |
| **核心主题** | AI Agent 评估方法论与工程实践 |
| **目标读者** | AI Agent 开发者、ML 工程师、产品经理 |
| **阅读价值** | ⭐⭐⭐⭐⭐ 官方一手经验，系统全面，可直接落地 |

> 💡 **一句话总结**：评估不是事后检验，而是 AI Agent 开发的「方向盘」——没有评估就是蒙眼开车，有了评估才能快速迭代、安全升级。

---

## 🗺️ 知识地图

```
┌─────────────────────────────────────────────────────────────┐
│              揭秘 AI Agents 评估                             │
├─────────────────────────────────────────────────────────────┤
│  一、评估的基本结构                                          │
│      └─ 核心概念：Task、Trial、Grader、Transcript、Result    │
├─────────────────────────────────────────────────────────────┤
│  二、为什么要构建评估？⭐                                     │
│      └─ 避免被动循环、加速开发、明确标准、快速升级            │
├─────────────────────────────────────────────────────────────┤
│  三、不同类型 Agent 的评估方法 ⭐⭐                           │
│      ├─ 编码智能体：SWE-bench、Terminal-Bench               │
│      ├─ 对话智能体：τ-Bench、多维度评估                      │
│      ├─ 研究智能体：BrowseComp、来源质量                     │
│      └─ 计算机使用智能体：WebArena、OSWorld                  │
├─────────────────────────────────────────────────────────────┤
│  四、处理非确定性：pass@k vs pass^k                          │
├─────────────────────────────────────────────────────────────┤
│  五、从零构建评估的 8 个步骤 ⭐⭐⭐                           │
├─────────────────────────────────────────────────────────────┤
│  六、评估与其他方法的协同（瑞士奶酪模型）                     │
└─────────────────────────────────────────────────────────────┘
```

---

## 🔑 核心概念体系

### 评估的基本定义

| 概念 | 定义 | 为什么重要 |
|------|------|------------|
| **Task（任务）** | 单个测试用例，有明确输入和成功标准 | 评估的最小单元 |
| **Trial（试验）** | 每个任务的一次尝试 | AI 非确定性，需多次运行 |
| **Grader（评分器）** | 评分逻辑，分代码/模型/人类三种 | 决定"好"的定义方式 |
| **Transcript（记录）** | 完整交互历史，含工具调用、推理过程 | 调试和理解失败的关键 |
| **Result（结果）** | 环境最终状态 | 评"做到了什么"而非"怎么做的" |

### 三种评分器对比

| 类型 | 特点 | 适用场景 |
|------|------|----------|
| **代码评分器** | 快速、客观、可重现 | 有明确正确答案（单元测试、静态分析） |
| **模型评分器** | 灵活、能处理主观任务 | 语气、质量等软性指标 |
| **人类评分器** | 黄金标准但昂贵缓慢 | 校准其他评分器、处理边界案例 |

### 两种评估类型

| 类型 | 目标 | 通过率起点 | 说明 |
|------|------|------------|------|
| **能力评估** | "智能体能做什么？" | 低 | 推动团队挑战难题 |
| **回归评估** | "还能做以前能做的吗？" | 近 100% | 防止功能退化 |

---

## 📊 处理非确定性：pass@k vs pass^k

### 定义

- **pass@k**：k 次尝试中**至少成功一次**的概率 → k 越大，分数越高
- **pass^k**：k 次尝试**全部成功**的概率 → k 越大，分数越低

### 选择逻辑

| 场景类型 | 推荐指标 | 原因 |
|----------|----------|------|
| 辅助工具（人可介入） | pass@k | 用户可以重试或人工干预 |
| 自主执行（不可逆） | pass^k | 必须高度可靠，错误代价大 |
| 代码生成 | pass@k | 开发者可以审查和修改 |
| 自动转账 | pass^k | 绝对不能出错 |

### 数学直觉

单次成功率 80% 的 Agent：
- pass@3 ≈ 99.2%（3 次里总能成功一次）
- pass^3 ≈ 51.2%（3 次全成功概率骤降）

---

## 🎯 不同类型 Agent 的评估策略

### 1. 编码智能体

| 维度 | 方法 |
|------|------|
| 结果正确性 | 单元测试验证代码 |
| 代码质量 | 静态分析检查 |
| 行为规范 | 检查工具调用与交互记录 |

**代表 Benchmark**：
- SWE-bench Verified：通过率从 40% 升至 80%+
- Terminal-Bench：测试构建内核等复杂任务

### 2. 对话智能体

| 评估维度 | 说明 |
|----------|------|
| 状态达成 | 是否完成用户目标 |
| 轮次控制 | 交互效率 |
| 语气合规 | 是否符合品牌要求 |

**代表方法**：τ-Bench / τ2-Bench（LLM 模拟用户多轮交互）

### 3. 研究智能体

| 评估维度 | 说明 |
|----------|------|
| 基础检查 | 事实是否正确 |
| 覆盖验证 | 信息是否全面 |
| 来源质量 | 引用是否权威 |
| 连贯性 | 逻辑是否自洽 |

**代表 Benchmark**：BrowseComp（测"大海捞针"信息检索）

### 4. 计算机使用智能体

| 评估维度 | 说明 |
|----------|------|
| 最终状态 | 订单是否真的创建 |
| 令牌效率 | 操作是否精简 |
| 执行速度 | 延迟是否可接受 |

**代表 Benchmark**：
- WebArena：浏览器任务
- OSWorld：操作系统级任务

---

## 🚀 从零构建评估的 8 个步骤

### 收集初始数据集

| 步骤 | 要点 | 时间投入 |
|------|------|----------|
| **0. 尽早开始** | 20-50 个真实失败案例即可起步，不要等"完美" | - |
| **1. 从手动测试挖用例** | 把现有验证流程自动化，按用户影响排序 | 1-2 小时 |
| **2. 明确任务** | 确保"双专家可复判"——两个专家独立达成相同判断 | 2-3 小时 |
| **3. 平衡测试集** | 既测"应该做"，也测"不应做"的场景 | 1 小时 |

### 设计评估框架

| 步骤 | 要点 |
|------|------|
| **4. 隔离干净环境** | 禁残留文件/缓存/资源泄漏，防止"git 偷看"类作弊 |
| **5. 评结果不评路径** | 多给部分分；LLM 评分须与人校准；留"未知"出口 |

### 长期维护

| 步骤 | 要点 |
|------|------|
| **6. 定期读日志** | 确认失败公平，排除评分器 bug |
| **7. 监控饱和** | 通过率 >80% 时换更难任务，避免"小步美化"掩盖真实提升 |
| **8. 评估先行** | 先定义成功标准，再开发功能；PR 式开放提交评估用例 |

---

## 🧀 瑞士奶酪模型：多层防护

```
┌─────────────────────────────────────────────────┐
│                    问题                          │
│                      ▼                          │
│  ┌─────────────────────────────────────────┐    │
│  │      自动化评估（CI/CD）                 │    │
│  │         ○  ○     ○    ○               │ ← 有些边界情况漏过
│  └─────────────────────────────────────────┘    │
│                      ▼                          │
│  ┌─────────────────────────────────────────┐    │
│  │      生产监控（发布后检测漂移）           │    │
│  │       ○       ○    ○       ○           │ ← 有些异常难以定义
│  └─────────────────────────────────────────┘    │
│                      ▼                          │
│  ┌─────────────────────────────────────────┐    │
│  │      用户反馈 + 日志审查                  │    │
│  │            ○       ○                    │ ← 覆盖率有限
│  └─────────────────────────────────────────┘    │
│                      ▼                          │
│  ┌─────────────────────────────────────────┐    │
│  │      人工研究（校准 LLM 评分器）          │    │
│  │                 ○                       │ ← 成本高，不能全覆盖
│  └─────────────────────────────────────────┘    │
│                      ▼                          │
│                 问题被拦截 ✅                    │
└─────────────────────────────────────────────────┘
```

### 各层职责

| 层级 | 时机 | 作用 |
|------|------|------|
| 自动化评估 | 发布前 CI/CD | 拦截已知类型问题 |
| 生产监控 | 发布后 | 检测分布漂移、异常模式 |
| A/B 测试 | 有流量时 | 验证真实用户场景 |
| 用户反馈 | 持续 | 发现评估未覆盖的问题 |
| 人工审查 | 定期 | 校准自动评分、发现盲区 |

---

## 💡 核心洞见

1. **"没有评估就是被动循环"** — 问题只能在生产环境发现，修一个引一个
2. **"评估的价值在生命周期累积"** — 越早建立，收益越大
3. **"评结果不评路径"** — 允许 Agent 走不同路线达到同一目标
4. **"双专家可复判"原则** — 好的任务描述应让两个专家独立达成相同判断
5. **"瑞士奶酪模型"** — 多层防护，一层漏掉另一层捕捉

---

## ⚠️ 常见误区

| 误区 | 正确做法 |
|------|----------|
| ❌ 等待"完美"评估才开始 | ✅ 20-50 个案例即可起步，边用边完善 |
| ❌ 只测成功路径 | ✅ 正负样本平衡，也测"不该做"的场景 |
| ❌ 评分过于严格（只看完美） | ✅ 允许部分得分，更准确反映能力 |
| ❌ 只依赖自动化评估 | ✅ 结合生产监控、用户反馈、人工审查 |
| ❌ 评估一劳永逸 | ✅ 持续维护，定期更新任务难度 |

---

## ✅ 行动清单

### 立即可做（今天）
- [ ] 收集 20-50 个 Agent 的真实失败案例
- [ ] 为每个案例写明确的成功标准

### 短期实践（本周）
- [ ] 搭建评估基础设施：Task + Trial + Grader
- [ ] 选择适合场景的指标：pass@k 或 pass^k
- [ ] 建立第一版评估集（至少 50 个任务）

### 长期提升（持续）
- [ ] 建立评估先行的开发文化
- [ ] 定期审查失败日志，确保评估公平
- [ ] 监控饱和度，>80% 时补充更难任务
- [ ] 结合多层防护：自动评估 + 监控 + 反馈

---

## 📚 延伸阅读

- [SWE-bench](https://www.swebench.com/) - 编码智能体评估标准
- [τ-Bench](https://github.com/sierra-research/tau-bench) - 对话智能体评估
- [WebArena](https://webarena.dev/) - 计算机使用智能体评估
- [OSWorld](https://os-world.github.io/) - 操作系统级智能体评估

---

## 📋 金句摘录

> "没有评估的团队陷入被动循环：只能在生产环境发现问题，修复一个 bug 又引入新的问题。"

> "评估迫使团队在开发前就定义'好'的具体含义。"

> "好的任务描述应该让两个专家独立达成相同的通过/失败判断。"

> "与安全工程中的瑞士奶酪模型一样，没有单一的评估层能捕捉到每个问题。"
