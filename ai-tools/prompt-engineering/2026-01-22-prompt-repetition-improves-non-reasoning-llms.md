# Prompt Repetition - 重复提示词让非推理模型更聪明

> 📅 学习日期：2026-01-22
> 📖 原文来源：Google Research 论文 arXiv:2512.14982
> 🏷️ 标签：#Prompt工程 #LLM #因果语言模型 #非推理模型 #论文精读

---

## 📖 论文概览

| 项目 | 内容 |
|------|------|
| **论文标题** | Prompt Repetition Improves Non-Reasoning LLMs |
| **作者** | Yaniv Leviathan, Matan Kalman, Yossi Matias (Google Research) |
| **发布时间** | 2025年12月17日 |
| **论文地址** | https://arxiv.org/abs/2512.14982 |
| **核心发现** | 简单重复 Prompt 一遍（Ctrl+C, Ctrl+V），非推理模型准确率显著提升 |

> **一句话总结**：LLM 是"只能向前看"的因果模型，重复 Prompt 等于给它一次"回头看"的机会。

---

## 🔬 实验设计与结果

### 测试模型

| 模型 | 厂商 | 类型 |
|------|------|------|
| Gemini 2.0 Flash / Lite | Google | 非推理 |
| GPT-4o / 4o-mini | OpenAI | 非推理 |
| Claude 3 Haiku / 3.7 Sonnet | Anthropic | 非推理 |
| DeepSeek V3 | DeepSeek | 非推理 |

### 测试基准

| 基准 | 类型 | 说明 |
|------|------|------|
| ARC | 科学推理 | 小学科学问题 |
| OpenBookQA | 常识推理 | 开放知识问答 |
| GSM8K | 数学 | 小学数学应用题 |
| MMLU-Pro | 综合 | 多领域专业知识 |
| MATH | 数学 | 竞赛数学 |
| **NameIndex** | 自创 | 给50个名字，问第25个是谁 |
| **MiddleMatch** | 自创 | 在重复名字列表中找中间那个 |

### 实验结果

```
70 组基准模型对比：
├── 重复提示赢：47 次
├── 平局：23 次
└── 输：0 次

最极端提升：21.33% → 97.33%（NameIndex 任务）
```

---

## 🧠 原理解析：为什么复制粘贴有效？

### 因果语言模型的"单向视野"限制

```
┌─────────────────────────────────────────────────────────────┐
│            因果语言模型 (Causal Language Model)              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   训练方式：从左到右，一个 token 一个 token 预测             │
│                                                             │
│   Token 1 → Token 2 → Token 3 → Token 4 → ...              │
│      ↓         ↓         ↓         ↓                        │
│   只能看    只能看     只能看     只能看                      │
│   自己      1         1,2       1,2,3                       │
│                                                             │
│   关键限制：当前 token 无法"回头看"后面的 token              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 字幕比喻

想象你在看视频字幕，字幕一行一行出现，无法回拉进度条：

**原始问题（单遍）**：
```
[选项 A、B] → [场景说明] → [问题]
     ↑
 读到选项时，还不知道场景是什么
 读到场景时，选项的"第一印象"已经定型
```

**重复问题（双遍）**：
```
[选项 A、B] → [场景] → [问题] → [选项 A、B] → [场景] → [问题]
                                     ↑
                             第二遍读选项时
                             脑子里已有完整的场景信息！
```

### 技术本质

| 单遍 `<Q>` | 双遍 `<Q1><Q2>` |
|-----------|-----------------|
| 每个 token 只能参考前面的 token | Q2 的每个 token 可以参考完整的 Q1 |
| 选项出现时，不知道场景 | 第二遍选项出现时，已知全部信息 |
| "第一印象"不完整 | "第一印象"被修正 |

---

## 🆚 为什么推理模型不需要这个技巧？

**关键洞察**：推理模型已经"自学"了这个技巧！

```
推理模型的典型行为：

用户："求解 x² + 2x - 3 = 0"

推理模型回答开头：
├── "题目问的是……"
├── "我们需要求解的是……"
├── "首先理解题目给出的条件……"
└── "让我重新表述一下问题……"

本质上 = 自动把问题复述一遍！
```

| 模型类型 | 是否需要手动重复 | 原因 |
|----------|-----------------|------|
| 非推理模型 | **需要** | 直接输出答案，没有"回头看"机会 |
| 推理模型 (o1, R1) | 不需要 | 通过 RL 微调学会了自动复述 |

**推理模型的代价**：速度慢！（思考可能要几分钟）

---

## 📊 与其他 Prompt 技巧对比

| 技巧 | 做法 | 原理 | 复杂度 | 额外成本 |
|------|------|------|--------|----------|
| **Prompt Repetition** | `<Q><Q>` | 利用因果注意力 | ⭐ | 输入 ×2 |
| **Chain-of-Thought** | `Let's think step by step` | 引导中间推理 | ⭐⭐ | 输出增加 |
| **Few-shot** | 给示例 | 建立模式 | ⭐⭐⭐ | 输入增加 |
| **Self-Consistency** | 多次采样取众数 | 统计投票 | ⭐⭐⭐⭐ | 多次调用 |
| **复杂 Prompt 模板** | role/rule/context/format | 结构化约束 | ⭐⭐⭐⭐⭐ | 设计成本 |

**关键区别**：
- 其他技巧：增加生成长度、增加计算量
- Prompt Repetition：**不增加生成长度、不增加延迟**，只增加输入长度

---

## ✅ 适用场景与限制

### 最适用场景

| 场景 | 为什么有效 |
|------|-----------|
| 选择题（选项在前、条件在后） | 重复后选项能"看到"条件 |
| 长问题（关键信息分散） | 整合分散信息 |
| 列表索引类任务 | 第二遍能参考完整列表 |
| 短问答（非推理模型） | 快速提升准确率 |

### 限制

| 限制 | 说明 |
|------|------|
| Token 消耗翻倍 | 输入长度 ×2 |
| 推理模型无效 | 已经自动做了 |
| 超长文本可能无效 | 重复后可能超出上下文窗口 |
| 不是万能药 | 某些任务提升不明显 |

---

## 🔮 未来研究方向

| 方向 | 设想 |
|------|------|
| **训练集成** | 在预训练/微调时加入重复结构 |
| **KV Cache 优化** | 只保留第二遍的 KV，减少推理开销 |
| **部分重复** | 只重复关键部分，而非全文 |
| **多模态重复** | 图像、视频的重复机制 |

---

## 🔑 核心洞见

| 维度 | 洞见 |
|------|------|
| **底层原理** | 因果语言模型的单向注意力限制 |
| **技术本质** | 重复 = 给后续 token 看到完整前文的机会 |
| **方法价值** | 零成本（只需 Ctrl+V）、立即可用、效果显著 |
| **认知冲击** | Prompt 工程不需要那么"玄学" |
| **与推理模型的关系** | 推理模型的"复述习惯"本质相同 |

---

## 💡 可迁移的设计模式

### 1. "重复即强化"模式

| 领域 | 表现 |
|------|------|
| 人类学习 | 重复阅读、复述、背诵 |
| AI 推理 | 自动复述问题 |
| 情感构建 | 反复表达（"你已经做得很好了"） |

### 2. "利用架构特性"模式

不是对抗模型限制，而是利用它：
- 因果注意力是"限制"，但重复把它变成"特性"

### 3. "简单即有效"模式

- 最简单的方法往往被忽视
- "大力出奇迹"有时真的有效

---

## 📝 人文洞察

> "人类的很多情感，其实都是靠重复才能构筑的。爱一个人是日常的复制粘贴，专业是一辈子的复制粘贴，写作是对一些想法一遍又一遍的复制粘贴。"

| 人类 | AI |
|------|-----|
| 重复阅读加深理解 | 重复 Prompt 提升准确率 |
| 反复练习形成肌肉记忆 | 重复给模型"第二次机会" |
| 情感需要反复表达才能构建 | 信息需要重复才能被充分利用 |

---

## ✅ 行动清单

- [ ] 对简短问答任务尝试重复 Prompt（非推理模型）
- [ ] 选择题将选项放后面或重复整题
- [ ] 批量 API 调用测试重复效果
- [ ] 重新审视"复杂 Prompt"的必要性

---

## 📚 延伸阅读

- [论文原文](https://arxiv.org/abs/2512.14982)
- Chain-of-Thought Prompting
- Self-Consistency in LLMs
