---
title: Prompt Repetition 让非推理模型更聪明
source: https://arxiv.org/abs/2512.14982
author: Yaniv Leviathan, Matan Kalman, Yossi Matias (Google Research)
date: 2026-01-22
category: ai-tools
subcategory: prompt-engineering
tags: [Prompt工程, LLM, 因果语言模型, 非推理模型, 论文精读]
---

# Prompt Repetition：重复一遍就能让 LLM 更准确

> 原文：[Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
> 学习日期：2026-01-22
> 分类：ai-tools / prompt-engineering

---

## 根节点命题

> **LLM 是「只能向前看」的因果模型，重复 Prompt 等于给它一次「回头看」的机会**

**为什么这是根节点**：论文的所有发现——重复提升准确率、推理模型不需要重复、选择题效果最好——都可以从「因果语言模型的单向注意力限制」推导出来。重复不是魔法，是**利用架构特性**。

---

## 表示空间

> **描述「Prompt 优化」问题的核心维度**

| 维度 | 含义 | 本文位置 |
|------|------|----------|
| 复杂度 | 技巧实施难度 | 极低（Ctrl+C, Ctrl+V） |
| 额外成本 | 增加的计算开销 | 输入 ×2，不增加生成长度和延迟 |
| 效果提升 | 准确率改善幅度 | 显著（最高 21%→97%） |
| 适用范围 | 哪些模型/任务有效 | 非推理模型 + 信息分散型任务 |

---

## 推论展开

> **从根节点推导出的核心结论**

```
根节点：LLM 只能向前看，重复 = 回头看的机会
│
├─ 推论1：选择题效果最好
│   └─ 原因：选项在前、条件在后，重复后选项能「看到」条件
│
├─ 推论2：推理模型不需要手动重复
│   └─ 原因：通过 RL 微调学会了自动复述（"让我理解一下题目..."）
│
├─ 推论3：长问题提升更明显
│   └─ 原因：信息越分散，重复整合的价值越大
│
└─ 推论4：是方法而非玄学
    └─ 原因：利用因果注意力的架构特性，可解释、可预测
```

---

## 泛化模式

> **这个洞见可以迁移到哪些其他场景？**

| 原场景 | 迁移场景 | 如何应用 |
|--------|----------|----------|
| Prompt 重复 | 人类学习 | 重复阅读、复述、背诵——原理相同 |
| Prompt 重复 | 情感构建 | 爱一个人是日常的复制粘贴 |
| 利用架构特性 | 任何技术优化 | 不是对抗限制，而是利用限制 |
| 简单即有效 | 产品设计 | 最简单的方法往往被忽视 |

---

## 关键概念

> **只保留理解根节点必需的概念**

| 概念 | 定义 | 与根节点的关系 |
|------|------|----------------|
| 因果语言模型 | 从左到右，每个 token 只能看前面的 token | 单向注意力是重复有效的根本原因 |
| 非推理模型 | 直接输出答案，不展开思考过程 | 重复技巧的目标模型 |
| 推理模型 | o1/R1 等，自动展开推理步骤 | 已经「自学」了重复技巧 |

---

## 实验结果

**70 组基准模型对比**：
- 重复提示赢：47 次
- 平局：23 次
- 输：0 次

**最极端提升**：NameIndex 任务 21.33% → 97.33%

### 测试模型覆盖

| 模型类别 | 代表模型 | 效果 |
|----------|----------|------|
| 通用模型 | GPT-4, Claude 3, Gemini | ✅ 有效 |
| 开源模型 | Llama 2, Mistral | ✅ 有效 |
| 推理模型 | o1, DeepSeek R1 | ❌ 无需重复（已内化） |

### 重复方式变体

| 变体 | 做法 | 效果 |
|------|------|------|
| **标准重复** | `<问题><问题>` | 基准效果 |
| **Verbose 重复** | `下面是问题：<问题>。为确保理解，我再说一遍：<问题>` | 略优于标准 |
| **×3 重复** | `<问题><问题><问题>` | 边际收益递减 |

**结论**：×2 是最佳性价比，×3 收益不明显。

---

## 何时使用

| 场景 | 是否推荐 | 原因 |
|------|----------|------|
| **选择题/匹配题** | ✅ 强烈推荐 | 效果最明显（选项在前、条件在后） |
| **长问题/信息分散** | ✅ 推荐 | 信息越分散，整合价值越大 |
| **短问答** | ⚠️ 可选 | 有提升但不如长问题显著 |
| **使用推理模型** | ❌ 不需要 | o1/R1 已内化此技巧 |
| **API 成本敏感** | ⚠️ 权衡 | 输入 Token ×2，但无生成延迟 |

---

## 与其他技巧对比

| 技巧 | 做法 | 复杂度 | 额外成本 |
|------|------|--------|----------|
| **Prompt Repetition** | `<Q><Q>` | ⭐ | 输入 ×2 |
| Chain-of-Thought | `Let's think step by step` | ⭐⭐ | 输出增加 |
| Few-shot | 给示例 | ⭐⭐⭐ | 输入增加 |
| Self-Consistency | 多次采样取众数 | ⭐⭐⭐⭐ | 多次调用 |

**关键区别**：重复**不增加生成长度、不增加延迟**，只增加输入长度。

---

## 行动清单

- [ ] 对简短问答任务尝试重复 Prompt（非推理模型）
- [ ] 选择题将选项放后面或重复整题
- [ ] 批量 API 调用测试重复效果
- [ ] 重新审视「复杂 Prompt」的必要性

---

## 知识关联

| 历史笔记 | 关系类型 | 关联说明 |
|----------|----------|----------|
| Context Engineering 系列 | 深化 | 重复是 Context 优化的最简单形式 |
| Agent Skills 系列 | 应用 | Skill 中的 Prompt 可以考虑重复技巧 |

**知识网络**：
```
本文：Prompt Repetition 技巧
│
├─ 深化：Context Engineering → 理解 LLM 如何处理上下文
└─ 应用：Agent Skills → Skill Prompt 设计可用此技巧
```

---

## 个人思考

{留空，供用户后续补充}

---

## 延伸阅读

- [论文原文](https://arxiv.org/abs/2512.14982)
- Chain-of-Thought Prompting
- Self-Consistency in LLMs
