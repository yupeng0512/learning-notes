# AGI-Next 前沿峰会 - 清华聚首中国 AI 基模四杰

> 精读日期：2026-01-16
> 原文来源：AGI-Next 前沿峰会实录
> 分类：AI 工具与效率

---

## 📖 快速概览

| 项目 | 内容 |
|------|------|
| **活动名称** | AGI-Next 前沿峰会 |
| **主办方** | 清华大学基础模型北京市重点实验室 |
| **核心嘉宾** | 唐杰（智谱）、杨植麟（Kimi）、林俊旸（阿里千问）、姚顺雨（腾讯/远程）、杨强（主持）、张钹院士 |
| **核心主题** | 从 Chat 到 Agent、自主学习、中美竞争 |

> 💡 **一句话总结**：这场峰会汇聚了中国 AI 基模四杰，深度讨论了大模型从"对话"走向"做事"的范式转变，揭示了 Scaling Law 的边界、自主学习的路径，以及中国 AI 反超的可能性与挑战。

---

## 🎯 四位大咖的核心观点速览

| 嘉宾 | 身份 | 核心金句 |
|------|------|----------|
| **唐杰** | 智谱 CEO | "DeepSeek 横空出世后，Chat 已经基本结束了，下一步是走向做事" |
| **杨植麟** | Kimi 创始人 | "做模型，本质上是在创造一种世界观" |
| **林俊旸** | 阿里千问负责人 | "中国想在 AI 赛道反超，很难。20% 这个数字已经很乐观" |
| **姚顺雨** | 腾讯/OpenAI 校友 | "toC 的话，大部分人其实用不着那么强的智能" |

---

## 🧠 唐杰（智谱）：让机器像人一样思考

### 核心理念

> "像喝咖啡一样做研究" —— 研究需要长期专注和持续投入

### 大模型发展脉络

```
2020 ──→ MMU/QA 简单问题
   ↓
2021-22 ──→ 数学计算、基础推理
   ↓
2023-24 ──→ 复杂推理、研究生级问题、SWE-bench
   ↓
2025 ──→ HLE（人类终极测试）、真实工作场景
```

### Chat 已死，Agent 当立

| 阶段 | 特征 | 智谱判断 |
|------|------|----------|
| **Chat 时代** | 对话交互为主 | 已结束（DeepSeek 后） |
| **Agent 时代** | 让 AI 完成具体事情 | 正在开启 |

**转折点判断**：

> "DeepSeek 横空出世，当时我们在内部反复讨论：Chat 这一代问题基本已经被解决得差不多。继续优化，大概率也只是做到性能接近。"

### RLVR 的挑战

| 领域 | 可验证性 | 挑战 |
|------|----------|------|
| **数学/编程** | 易定义 | 已有进展 |
| **网页美观/交互** | 难定义 | 仍需人工判断 |
| **物理世界任务** | 更难 | 环境构建复杂 |

> "可验证场景正在逐渐耗尽。接下来能否进入半自动验证甚至不可验证的任务空间，是关键问题。"

### 下一代 AGI 范式（五层思考）

```
层级1：系统一（F-X→X）──→ 自监督多任务学习
  ↓
层级2：系统二 ──→ 推理、知识融合
  ↓
层级3：自反思与自学习 ──→ 自我评估与批判
  ↓
层级4：自我认知 ──→ 解释行为、理解目标
  ↓
层级5：意识 ──→ "也许有一天，AI 也有意识"
```

### 三个 Scaling 方向

| Scaling 类型 | 内容 |
|--------------|------|
| **Scaling 数据与模型** | 提升智能上限 |
| **Scaling 推理** | 更长思考时间，更多计算与搜索 |
| **Scaling 自学习环境** | 更多与外界交互机会 |

### 2026 年展望

- **Scaling 继续**：已知路径 + 未知新范式探索
- **技术创新**：超长上下文、高效知识压缩、持续学习
- **多模态感统**：真实工作环境中执行长链路任务
- **AI for Science**：可能成为重要突破年份

---

## 🚀 杨植麟（Kimi）：从 Scaling Law 到世界观

### 第一性原理：Scaling Law

> "从 2019 年到现在所有的大模型基本上基于同一个第一性原理——Scaling Law，把能源转化成智能。"

### 为什么 Transformer 胜出？

| 架构 | 短 Context（<100 Token） | 长 Context |
|------|--------------------------|------------|
| **LSTM** | 相当 | 明显落后 |
| **Transformer** | 相当 | 显著更好 |

**关键洞见**：

> "Transformer 的优势体现在当 Context 非常长的时候。这在 Agentic 时代会变得非常重要，因为很多 Agent 任务要求非常长的上下文。"

### 两个核心优化方向

| 方向 | 目标 | 意义 |
|------|------|------|
| **Token Efficiency** | 用更少 Token 达到同样效果 | 互联网存量数据有限，效率越高智能上限越高 |
| **Long Context** | 更长的 Context 处理能力 | 复杂 Agent 任务必须在超长 Context 下完成 |

### MUON 优化器：2 倍 Token Efficiency

| 优化器 | 类型 | Token Efficiency |
|--------|------|------------------|
| **Adam**（2014-至今） | 一阶优化器 | 基线 |
| **MUON**（Kimi 采用） | 二阶优化器 | **2 倍提升** |

**工程挑战**：MUON 训练过程中 Logit 会爆炸式增长

**解决方案**：QK-clip 动态裁剪

> "这张图是 2025 年见过最漂亮的东西，这个是世界上最美的东西。"（指完全平稳下降的 Loss 曲线）

### Kimi Linear：下一代架构

| 特性 | 全注意力 | 线性注意力（传统） | Kimi Linear |
|------|----------|-------------------|-------------|
| **复杂度** | O(N²) | O(N) | O(N) |
| **短程任务** | 好 | 差 | **更好** |
| **长程任务** | 好 | 明显差 | **更好** |
| **速度（100 万 Context）** | 基线 | 快 6-10x | 快 6-10x |

### 做模型的本质

> "做模型的过程本质上是在创造一种世界观。你觉得什么样的东西是好的，一个好的 AI 应该有什么样的表现，应该追求什么样的价值观。"

### 与 Kimi 的对话

> "它可能不是一个普通工具，而是可以提升人类文明上限的东西。虽然它有风险，但它的回答是我仍然会选择继续开发，因为放弃这个开发就意味着放弃人类文明上限。"

---

## 🌐 林俊旸（阿里千问）：Towards a Generalist Agent

### 核心转变

| 之前 | 之后 |
|------|------|
| F-X → Y（监督学习） | F-X → X（自监督多任务学习） |
| 传统标注 | 解决推理 + 解决评估 = 可以干任何事 |

### Qwen3 系列核心特点

| 特性 | 内容 |
|------|------|
| **Reasoning 能力** | 显著提升，AIME 可达 70 分 |
| **语言支持** | 119 种语言及方言 |
| **长文本** | 1M+ Context（内部已做到几个 M） |

### 多语言的初心

> "有一回我遇到韩国朋友，他说你们的模型根本就不懂任何的韩语。巴基斯坦的朋友不断跟我说你快点支持乌尔都语，我们真的没有大模型可以用了。"

### Coding 能力的重要性

**美国 vs 中国的差异**：

> "美国基本上全都是 Coding，在中国真的没有那么大。"

**SWE-bench 进展**：从 67 分做到 70+ 分

### 多模态发展路径

```
VL（视觉语言）──→ 操控手机/电脑
      ↓
Omni（全模态）──→ 听+说+看
      ↓
生成（图像/视频）──→ Image Edit + Generation
      ↓
VLA（视觉语言动作）──→ 具身智能
```

### 下一代模型方向

| 方向 | 内容 |
|------|------|
| **新架构** | Linear Context（与 Kimi 殊途同归） |
| **全模态** | 三进三出（文本+视觉+音频） |
| **Multi-turn RL** | 与环境反馈的长程推理 |
| **Embodied Reasoning** | 走向物理世界 |

### 对反超的冷静判断

> "中国想在 AI 赛道反超，很难。20% 这个数字已经很乐观了。"

> "今天我们捉襟见肘，光交付可能就已经占据了我们绝大部分的 Compute，这会是一个比较大的差异。"

---

## 💡 姚顺雨（腾讯）：toB vs toC 的分化

### toC vs toB 的根本差异

| 维度 | toC | toB |
|------|-----|-----|
| **智能需求** | 大部分人用不着那么强的智能 | 智能越高，生产力越高 |
| **支付意愿** | 价格敏感 | 愿意为最强模型付溢价 |
| **感知差异** | ChatGPT 今年 vs 去年感受差别不大 | Coding 正在重塑整个行业 |

> "对于 toB 来说，很多人就愿意用最强的模型。一个 200 美元/月，差一些的 50 美元/月。问题是你不知道哪些任务做对了，需要花额外精力监控。"

### 垂直整合 vs 分层

| 领域 | 趋势 |
|------|------|
| **toC** | 垂直整合成立（ChatGPT/豆包） |
| **toB** | 分层可能更好（模型层 + 应用层） |

### 自主学习的定义问题

> "每个人对自主学习的定义和看法都不一样。这个事情不是方法论，而是数据或者任务。"

**已经发生的信号**：
- ChatGPT 利用用户数据弥合聊天风格
- Claude 已经写了 Claude 这个项目 95% 的代码
- Cursor 每几个小时用最新用户数据进行学习

### 中国的挑战

| 挑战 | 内容 |
|------|------|
| **光刻机** | 算力 Bottleneck |
| **toB 市场** | 支付意愿不足，大家都选择出海 |
| **冒险精神** | 更喜欢做确定性的事情 |

> "任何一个事情一旦被证明能做出来，很多人都会非常积极地尝试。但做前沿探索或者新范式突破的人可能还不够多。"

---

## 🔥 圆桌精华：四大核心议题

### 议题一：路线分化

| 观点来源 | 核心洞见 |
|----------|----------|
| **姚顺雨** | toC 和 toB 明显分化；垂直整合 vs 分层也在分化 |
| **林俊旸** | 模型即产品，服务真实问题才是关键 |
| **杨强** | 工业界和学术界的分化值得关注 |
| **唐杰** | Chat 已结束，下一仗是让 AI 做事 |

### 议题二：自主学习

| 观点来源 | 核心洞见 |
|----------|----------|
| **姚顺雨** | 已经在发生，但受限于想象力——它应该是什么样的效果？ |
| **林俊旸** | AI 能不能实现更强的主动性？同时要担心安全问题 |
| **杨强** | 联邦学习视角：通用大模型与本地小模型协作 |
| **唐杰** | 2026 年一定会有范式革新，要定义"Intelligence Efficiency" |

### 议题三：Agent 之年

| 观点来源 | 核心洞见 |
|----------|----------|
| **姚顺雨** | toB Agent 正在不断上升；除了模型，环境和部署是关键 |
| **林俊旸** | 未来 Agent 需要与物理世界交互，AGI 的魅力在于解决长尾问题 |
| **杨强** | Agent 有四个阶段：目标定义→规划→功能主体→意识主体 |
| **唐杰** | Agent 的价值取决于能否真正帮到人，以及 Cost 是否可接受 |

### 议题四：中国能否反超？

| 观点来源 | 概率判断 | 关键条件 |
|----------|----------|----------|
| **姚顺雨** | 挺高（乐观） | 光刻机突破 + toB 市场成熟 + 更多冒险精神 |
| **林俊旸** | **20%**（已经很乐观） | 穷则思变，软硬结合可能是机会 |
| **杨强** | 可能（类比互联网） | toC 百花齐放，toB 会跟上 |
| **唐杰** | 有信心 | 聪明人敢冒险 + 环境改善 + 笨笨的坚持 |

---

## 🎓 张钹院士：AI 时代的企业家使命

### 分布式语义的五个缺失

| 缺失 | 影响 |
|------|------|
| **指称缺失** | 无法精确指向现实世界 |
| **真知和因果缺失** | 无法理解因果关系 |
| **语用缺失** | 无法理解语境和意图 |
| **多义和动态语境缺失** | 无法处理歧义 |
| **闭环行为缺失** | 无法自我验证 |

### AGI 的五个关键能力（可执行、可检验的定义）

| 能力 | 关键形容词 |
|------|------------|
| **多模态理解与落地** | 时空一致的 |
| **在线学习与适应** | 可控的 |
| **推理与长期规划** | 可验证的 |
| **反思与元认知** | 可校准的 |
| **跨任务泛化** | 强泛化（分布外、长尾） |

### 未来主体的三个层次

| 层次 | 状态 | 担忧程度 |
|------|------|----------|
| **功能-行动主体** | 已达到 | 希望达到 |
| **规范-责任主体** | 未达到，技术难度高 | 希望达到 |
| **体验-意识主体** | 未达到 | 最担忧 |

### AI 时代企业家的新定义

> "大模型之后，我觉得最优秀的学生应该去搞企业。因为人工智能给企业家做的重新的定义。"

**六大职责**：
1. 重新定义价值创造
2. 把 AI 变成像水和电那样的通用技术
3. 担负社会责任（治理）
4. 把知识、伦理和应用变成可复用工具
5. 实现对人类的造福
6. 成为光荣、神圣的职业

---

## 📊 核心观点对比表

| 议题 | 唐杰 | 杨植麟 | 林俊旸 | 姚顺雨 |
|------|------|--------|--------|--------|
| **Chat 时代** | 已结束 | - | - | - |
| **下一代重点** | Agent + Coding | Token Efficiency + Long Context | Generalist Agent | toB 生产力 |
| **架构创新** | 探索新范式 | Kimi Linear | Linear Context | - |
| **自主学习** | 2026 年会有范式革新 | 更多 Taste | 主动性 + 安全 | 已在发生 |
| **中国反超** | 有信心，需坚持 | - | 20% | 挺高，需冒险精神 |

---

## ⚡ 关键技术趋势总结

### 1. 从 Chat 到 Agent 的范式转变

```
Chat（对话）──→ Agent（做事）──→ 自主学习（自我进化）
    ↓                ↓                  ↓
  已结束          正在发生           2026 年关键
```

### 2. Scaling Law 的三个方向

```
Scaling 数据/模型 ──→ 智能上限
Scaling 推理 ──→ 思考深度
Scaling 自学习环境 ──→ 真实世界交互
```

### 3. 架构演进

```
Transformer ──→ 线性注意力（Kimi Linear/千问 Linear Context）
    ↓
O(N²) ──→ O(N)，100 万 Context 快 6-10 倍
```

### 4. 多模态融合路径

```
文本 ──→ 视觉 ──→ 音频 ──→ 生成 ──→ 具身
 LLM    VLM    Omni   Image/Video  VLA
```

---

## 📝 金句摘录

> "DeepSeek 横空出世后，Chat 已经基本结束了，下一步是走向做事。" —— 唐杰

> "做模型，本质上是在创造一种世界观。" —— 杨植麟

> "中国想在 AI 赛道反超，很难。20% 这个数字已经很乐观。" —— 林俊旸

> "toC 的话，大部分人其实用不着那么强的智能。" —— 姚顺雨

> "放弃这个开发就意味着放弃人类文明上限。" —— Kimi（与杨植麟对话）

> "AI 时代，企业家会变成光荣的、神圣的职业之一。" —— 张钹院士

> "我们不再缺能写代码的 AI，我们缺的是能遵守工程规范的 AI。" —— 呼应 Agent 纪律性

---

## ✅ 行动清单

### 立即可做（今天）
- [ ] 理解核心转变：**Chat 已结束，Agent 当立**
- [ ] 关注 Token Efficiency 和 Long Context 两个优化方向

### 短期实践（本周）
- [ ] 体验 Kimi K2、智谱 GLM、千问 Qwen3 的 Agent 能力
- [ ] 关注 MUON 优化器和线性注意力架构的开源进展

### 长期思考
- [ ] 如何定义"Intelligence Efficiency"——获得智能的效率
- [ ] 自主学习的具体形态：什么任务？什么奖励函数？什么效果？
- [ ] 中国 AI 的机会：软硬结合、穷则思变、长尾问题

---

## 🔗 相关资源

- [智谱 GLM 开源系列](https://github.com/THUDM)
- [Kimi K2](https://kimi.moonshot.cn)
- [阿里千问 Qwen](https://github.com/QwenLM)
- [清华大学基础模型北京市重点实验室](https://ml.cs.tsinghua.edu.cn/)
- [DeepSeek](https://www.deepseek.com/)

---

## 个人思考

{留空，供后续补充}
